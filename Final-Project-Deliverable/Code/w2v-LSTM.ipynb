{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYPhnopaWVoU",
        "outputId": "5f8cbdcc-3403-4be1-f482-64552ceb8359"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 15 13:47:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Arabic-Stopwords\n",
        "!pip install arabic_reshaper\n",
        "!pip install python-bidi\n",
        "!pip install tensorflow-addons #to use f1 score in complie's metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtDpld5jWxKS",
        "outputId": "d112b26f-3516-451c-e35a-b8974e903f5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Arabic-Stopwords\n",
            "  Downloading Arabic_Stopwords-0.3-py3-none-any.whl (353 kB)\n",
            "\u001b[K     |████████████████████████████████| 353 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting pyarabic>=0.6.2\n",
            "  Downloading PyArabic-0.6.14-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 33.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic>=0.6.2->Arabic-Stopwords) (1.15.0)\n",
            "Installing collected packages: pyarabic, Arabic-Stopwords\n",
            "Successfully installed Arabic-Stopwords-0.3 pyarabic-0.6.14\n",
            "Collecting arabic_reshaper\n",
            "  Downloading arabic_reshaper-2.1.3-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from arabic_reshaper) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from arabic_reshaper) (57.4.0)\n",
            "Installing collected packages: arabic-reshaper\n",
            "Successfully installed arabic-reshaper-2.1.3\n",
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-bidi) (1.15.0)\n",
            "Installing collected packages: python-bidi\n",
            "Successfully installed python-bidi-0.4.2\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FrW_MDPzkpYn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM\n",
        "import tensorflow_addons as tfa\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Large Multi-Domain Resources for Arabic Sentiment Analysis**"
      ],
      "metadata": {
        "id": "4pk48tpxATRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this dataset in MSA, I will use w2v pretrained on Wikipedia."
      ],
      "metadata": {
        "id": "yYBabBOZAbns"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0z2fXWt9UOtx"
      },
      "outputs": [],
      "source": [
        "#read files of first dataset (MSA)\n",
        "res_df=pd.read_csv('https://raw.githubusercontent.com/hadyelsahar/large-arabic-sentiment-analysis-resouces/master/datasets/RES.csv')\n",
        "prod_df=pd.read_csv('https://raw.githubusercontent.com/hadyelsahar/large-arabic-sentiment-analysis-resouces/master/datasets/PROD.csv')\n",
        "htl_df=pd.read_csv('https://raw.githubusercontent.com/hadyelsahar/large-arabic-sentiment-analysis-resouces/master/datasets/HTL.csv')\n",
        "mov_df=pd.read_csv('https://raw.githubusercontent.com/hadyelsahar/large-arabic-sentiment-analysis-resouces/master/datasets/MOV.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ufd6aMxCUd_l"
      },
      "outputs": [],
      "source": [
        "#keep only binary classes (pos & neg)\n",
        "res_df=res_df[res_df['polarity']!=0].reset_index(drop=True)\n",
        "prod_df=prod_df[prod_df['polarity']!=0].reset_index(drop=True)\n",
        "htl_df=htl_df[htl_df['polarity']!=0].reset_index(drop=True)\n",
        "mov_df=mov_df[mov_df['polarity']!=0].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XigvONztUhFF"
      },
      "outputs": [],
      "source": [
        "datasetDict = {\"resturants\": res_df, \"products\": prod_df, \"hotels\": htl_df, \"movies\": mov_df}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert neg label from -1 to 0\n",
        "for k, v in datasetDict.items():\n",
        "  v['polarity'].replace({-1: 0}, inplace=True)"
      ],
      "metadata": {
        "id": "cjbwAycJdV-D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N8gxfpnAvXxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "987f6b49-a3f3-4472-921b-55bb0d903be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import string\n",
        "import arabicstopwords.arabicstopwords as ar_words\n",
        "\n",
        "ar_sw=['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'إنها', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما','كان','كانت', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا']\n",
        "\n",
        "def normalizeArabic(t):\n",
        "    t = re.sub(\"[إأٱآا]\", \"ا\", t)\n",
        "    t = re.sub(\"ى\", \"ي\", t)\n",
        "    t = re.sub(\"ة\", 'ه', t)\n",
        "    t = re.sub(\"ؤ\", \"ء\", t)\n",
        "    t = re.sub(\"ئ\", \"ء\", t)\n",
        "    return (t)\n",
        "\n",
        "ar_stop= []\n",
        "for w in ar_sw:\n",
        "  ar_stop.append(normalizeArabic(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNVSWtvmvY70",
        "outputId": "c3de8fb5-1fd8-45a5-df21-a6a1f7095f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nlkt arabic stopwords = 754\n",
            "Arabic-Stopwords = 13629\n",
            "My list = 251\n",
            "sum = 14634 unique= 13997\n"
          ]
        }
      ],
      "source": [
        "#COLLECT Ar stopwords from 2 sources\n",
        "\n",
        "ar_stopwords = stopwords.words('arabic') + list(ar_words.stopwords_list()) + ar_stop\n",
        "print('nlkt arabic stopwords =',len(stopwords.words('arabic')))\n",
        "print('Arabic-Stopwords =', len(ar_words.stopwords_list()))\n",
        "print('My list =', len(ar_stop))\n",
        "print('sum =',len(ar_stopwords), 'unique=', len(set(ar_stopwords)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GCtRQLszv0zM"
      },
      "outputs": [],
      "source": [
        "# define 'clean_tweet' function to clean the text and remove unwanted text parts\n",
        "def clean_text(text):\n",
        "    # define regular expression patterns\n",
        "    p_english = \"[a-zA-Z0-9]+\"\n",
        "    p_url = \"https?://[A-Za-z0-9./]+\"\n",
        "    p_mention = \"\\@[\\_0-9a-zA-Z]+\\:?\"    \n",
        "    p_retweet = \"RT \\@[\\_\\-0-9a-zA-Z]+\\:?\"\n",
        "    p_punctuations = \"[\" + string.punctuation + \"]\"\n",
        "    \n",
        "    # remove unwanted parts\n",
        "    text = re.sub(p_english, ' ', text)\n",
        "    text = re.sub(p_retweet, ' ', text)\n",
        "    text = re.sub(p_mention, ' ', text)\n",
        "    text = re.sub(p_url, ' ', text)\n",
        "    text = re.sub(p_punctuations, ' ', text)\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    \n",
        "    # remove الهمزة\n",
        "    text = re.sub(\"[أإآ]\", 'ا', text)\n",
        "    text = re.sub(\"ة\", 'ه', text)\n",
        "    text = re.sub(\"ى\", 'ي', text)\n",
        "    \n",
        "    # removing tashkeel\n",
        "    tashkel = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(tashkel, '', text)\n",
        "    \n",
        "    # remove repeated letters more than two letters\n",
        "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "    \n",
        "    #trim    \n",
        "    text = text.strip()\n",
        "    \n",
        "    # remove stopwords\n",
        "    words = [word for word in text.split() if word not in ar_stopwords]\n",
        "    words = [word for word in words if len(word)>=2]\n",
        "    \n",
        "    # merge and return final text\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9nLcPPX2iAIT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from nltk import ngrams\n",
        "\n",
        "# Clean/Normalize Arabic Text\n",
        "def clean_str(text):\n",
        "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
        "    \n",
        "    #remove tashkeel\n",
        "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    text = re.sub(p_tashkeel,\"\", text)\n",
        "    \n",
        "    #remove longation\n",
        "    p_longation = re.compile(r'(.)\\1+')\n",
        "    subst = r\"\\1\\1\"\n",
        "    text = re.sub(p_longation, subst, text)\n",
        "    \n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "    \n",
        "    for i in range(0, len(search)):\n",
        "        text = text.replace(search[i], replace[i])\n",
        "    \n",
        "    #trim    \n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3ofazvFKus-n"
      },
      "outputs": [],
      "source": [
        "# apply clean function on all text in the dataframes\n",
        "for k, v in datasetDict.items():\n",
        "    v['clean_text'] = v['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wIIpwzkmP7R",
        "outputId": "6b8a07bd-4b35-405e-a9f9-09c4b2814afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-15 14:19:10--  https://bakrianoo.ewr1.vultrobjects.com/aravec/full_uni_sg_300_wiki.zip\n",
            "Resolving bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)... 108.61.0.122, 2001:19f0:0:22::100\n",
            "Connecting to bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)|108.61.0.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 720163266 (687M) [application/zip]\n",
            "Saving to: ‘full_uni_sg_300_wiki.zip’\n",
            "\n",
            "full_uni_sg_300_wik 100%[===================>] 686.80M  94.5MB/s    in 7.3s    \n",
            "\n",
            "2021-12-15 14:19:18 (93.7 MB/s) - ‘full_uni_sg_300_wiki.zip’ saved [720163266/720163266]\n",
            "\n",
            "Archive:  full_uni_sg_300_wiki.zip\n",
            "  inflating: full_uni_sg_300_wiki.mdl  \n",
            "  inflating: full_uni_sg_300_wiki.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_sg_300_wiki.mdl.wv.vectors.npy  \n"
          ]
        }
      ],
      "source": [
        "!wget 'https://bakrianoo.ewr1.vultrobjects.com/aravec/full_uni_sg_300_wiki.zip'\n",
        "!unzip 'full_uni_sg_300_wiki.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2wmOkmtsCZr",
        "outputId": "4598e88b-b916-4f61-887d-29ae72253122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We've 320636 vocabularies\n"
          ]
        }
      ],
      "source": [
        "# load the AraVec model\n",
        "w2v = gensim.models.Word2Vec.load(\"full_uni_sg_300_wiki.mdl\")\n",
        "print(\"We've\",len(w2v.wv.index2word),\"vocabularies\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_fAz-njyw8X",
        "outputId": "98eeec76-cf21-49b4-ac09-1bb7c718ac5d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "one training text after padding: \n",
            "[27250   150 10991    30    12   191   894   189   499   302  4058 27251\n",
            "   409   183  2088   732   183  2892   145   326  1907 11482     9     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#embedding matrix shape (vocab, embeddings dim) is  (46498, 300)\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 128, 300)          13949400  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              186880    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,140,441\n",
            "Trainable params: 191,041\n",
            "Non-trainable params: 13,949,400\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 173s 979ms/step - loss: 1.8654 - acc: 0.7565 - f1_score: 0.8574 - val_loss: 0.4750 - val_acc: 0.7683 - val_f1_score: 0.8649\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 202s 1s/step - loss: 0.4492 - acc: 0.7853 - f1_score: 0.8717 - val_loss: 0.4618 - val_acc: 0.7865 - val_f1_score: 0.8688\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 121s 703ms/step - loss: 0.5143 - acc: 0.8024 - f1_score: 0.8782 - val_loss: 0.4483 - val_acc: 0.7936 - val_f1_score: 0.8728\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 150s 874ms/step - loss: 0.4031 - acc: 0.8175 - f1_score: 0.8853 - val_loss: 0.4260 - val_acc: 0.8136 - val_f1_score: 0.8833\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 124s 723ms/step - loss: 0.3762 - acc: 0.8312 - f1_score: 0.8928 - val_loss: 0.4201 - val_acc: 0.8113 - val_f1_score: 0.8774\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 123s 714ms/step - loss: 0.3607 - acc: 0.8414 - f1_score: 0.8985 - val_loss: 0.4069 - val_acc: 0.8225 - val_f1_score: 0.8871\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 122s 712ms/step - loss: 0.4951 - acc: 0.7858 - f1_score: 0.8670 - val_loss: 0.4540 - val_acc: 0.7809 - val_f1_score: 0.8689\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 122s 708ms/step - loss: 0.4204 - acc: 0.8011 - f1_score: 0.8796 - val_loss: 0.4477 - val_acc: 0.7950 - val_f1_score: 0.8738\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 120s 699ms/step - loss: 0.4085 - acc: 0.8091 - f1_score: 0.8824 - val_loss: 0.4417 - val_acc: 0.8006 - val_f1_score: 0.8742\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 122s 712ms/step - loss: 0.6732 - acc: 0.8101 - f1_score: 0.8803 - val_loss: 0.4507 - val_acc: 0.7959 - val_f1_score: 0.8707\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8db369d6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "\n",
        "#------------------------------------------------------ Choose a dataset df\n",
        "df= res_df\n",
        "#df= prod_df\n",
        "#df= htl_df\n",
        "#df= mov_df\n",
        "\n",
        "#################################### vectorizing cleaned text\n",
        "# fit tokenizer vocab \n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.clean_text)\n",
        "\n",
        "################################### standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "\n",
        "################################## indexing + padding\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "print('one training text after padding: ')\n",
        "print(train_text[10])\n",
        "\n",
        "################################### preparing embedding matrix\n",
        "embedding_dim = w2v.vector_size # w2v embedding dim\n",
        "word_index = tokenizer.word_index # vocab\n",
        "\n",
        "# use the gensim model to build a numpy array of embeddings,\n",
        "# we'll feed this array to the keras embeddings layer.\n",
        "# each row i of the array will correspond to the word token assigned to that value \n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = w2v[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except: # word in our data vocab is missing in w2v, will use 0 vector for that word\n",
        "        pass\n",
        "print('embedding matrix shape (vocab, embeddings dim) is ', embedding_matrix.shape)\n",
        "\n",
        "\n",
        "##################################### defining model\n",
        "\n",
        "inp = Input(shape=(seq_len,))\n",
        "x = Embedding(len(word_index) + 1,\n",
        "              embedding_dim,\n",
        "              weights=[embedding_matrix], # where we feed the pretrained vecs\n",
        "              trainable=False)(inp) # freeze these parameters in the model\n",
        "#recurrent_dropout=.1\n",
        "x = Bidirectional(LSTM(64, activation='relu'))(x)\n",
        "x = Dense(32)(x) # fully connected layer on top of the output of the bi-LSTM\n",
        "#x = Dropout(.3)(x)\n",
        "#x = Dense(20)(x) #added\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "\n",
        "print(NN.summary())\n",
        "\n",
        "######################################## compiling model\n",
        "threshold = 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "\n",
        "\n",
        "######################################## fitting model = training\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=10, batch_size=50, verbose=1)\n",
        "\n",
        "\n",
        "######################################### testing\n",
        "\n",
        "# This can be used if we have train, val, test sets\n",
        "# but since data size is not to large, I choose 2 splits only\n",
        "#result = the validation of last epoch\n",
        "\n",
        "#prob= NN.predict(val_text)\n",
        "#predections= [1 if p >= 0.5 else 0 for p in prob]\n",
        "#f1_score(y_val, predections)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob= NN.predict(val_text)\n",
        "predections= [1 if p >= 0.5 else 0 for p in prob]\n",
        "print('---------------------------RESULT------------------------')\n",
        "print('The model f1 score on validation set is :', f1_score(y_val, predections))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C1AIH9ClW_n",
        "outputId": "f704418a-2eb1-45ca-ae0b-a75691b5772d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------RESULT------------------------\n",
            "The model f1 score on validation set is : 0.8707482993197277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next fit for the same above settings shows a bit better result! "
      ],
      "metadata": {
        "id": "Bp3Yn3u2jxp6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcU2SMFVuSs6",
        "outputId": "e0222557-8dbb-4ccd-c66f-ae9096610ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 60s 331ms/step - loss: 0.5208 - acc: 0.7605 - f1_score: 0.8607 - val_loss: 0.4532 - val_acc: 0.7823 - val_f1_score: 0.8718\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 57s 331ms/step - loss: 0.3816 - acc: 0.8305 - f1_score: 0.8935 - val_loss: 0.3835 - val_acc: 0.8342 - val_f1_score: 0.8932\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 57s 329ms/step - loss: 4514.7544 - acc: 0.8512 - f1_score: 0.9047 - val_loss: 0.3753 - val_acc: 0.8417 - val_f1_score: 0.9006\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 57s 334ms/step - loss: 0.3134 - acc: 0.8663 - f1_score: 0.9141 - val_loss: 0.3673 - val_acc: 0.8389 - val_f1_score: 0.8974\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 57s 332ms/step - loss: 0.2940 - acc: 0.8761 - f1_score: 0.9201 - val_loss: 0.3754 - val_acc: 0.8375 - val_f1_score: 0.8918\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 57s 329ms/step - loss: 0.2720 - acc: 0.8859 - f1_score: 0.9261 - val_loss: 0.3662 - val_acc: 0.8440 - val_f1_score: 0.8987\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 57s 332ms/step - loss: 0.2561 - acc: 0.8901 - f1_score: 0.9287 - val_loss: 0.3899 - val_acc: 0.8482 - val_f1_score: 0.9036\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 56s 328ms/step - loss: 0.2248 - acc: 0.9075 - f1_score: 0.9395 - val_loss: 0.4062 - val_acc: 0.8440 - val_f1_score: 0.9004\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 57s 331ms/step - loss: 0.1947 - acc: 0.9204 - f1_score: 0.9477 - val_loss: 0.4314 - val_acc: 0.8482 - val_f1_score: 0.9033\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 57s 331ms/step - loss: 0.1704 - acc: 0.9310 - f1_score: 0.9546 - val_loss: 0.4630 - val_acc: 0.8421 - val_f1_score: 0.8976\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee93bf6910>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#without dropout + clean text ------------ resturant\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=10, batch_size=50, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "mpJ4L7aTgIKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are some past results with some different choices, for my own learning/comparing ..... "
      ],
      "metadata": {
        "id": "Kyj0V1EkfKaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCk1AVTsNI9x",
        "outputId": "e5f37e09-47d8-4cf9-81f6-8b39fb7cd700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "172/172 [==============================] - 96s 513ms/step - loss: 1.1893 - acc: 0.7738 - f1_score: 0.8652 - val_loss: 0.4601 - val_acc: 0.7833 - val_f1_score: 0.8719\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 88s 510ms/step - loss: 0.4338 - acc: 0.7884 - f1_score: 0.8743 - val_loss: 0.4449 - val_acc: 0.7931 - val_f1_score: 0.8738\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 89s 518ms/step - loss: 0.4406 - acc: 0.7932 - f1_score: 0.8744 - val_loss: 0.4670 - val_acc: 0.7660 - val_f1_score: 0.8598\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 89s 516ms/step - loss: 0.4339 - acc: 0.7883 - f1_score: 0.8702 - val_loss: 0.4629 - val_acc: 0.7716 - val_f1_score: 0.8602\n",
            "Epoch 5/5\n",
            "172/172 [==============================] - 89s 518ms/step - loss: 533584.6875 - acc: 0.7732 - f1_score: 0.8607 - val_loss: 1.3926 - val_acc: 0.7459 - val_f1_score: 0.8443\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee99f0c8d0>"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#with dropouts + clean text ------------ resturant\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=5, batch_size=50, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3qi7wFcVDot",
        "outputId": "19593b07-d96b-412e-a853-3e291f6d5b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "172/172 [==============================] - 57s 332ms/step - loss: 292.6279 - acc: 0.7621 - f1_score: 0.8618 - val_loss: 0.4804 - val_acc: 0.7623 - val_f1_score: 0.8628\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 57s 329ms/step - loss: 0.4330 - acc: 0.7987 - f1_score: 0.8779 - val_loss: 0.4916 - val_acc: 0.8178 - val_f1_score: 0.8839\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 57s 331ms/step - loss: 0.3914 - acc: 0.8286 - f1_score: 0.8922 - val_loss: 0.4066 - val_acc: 0.8234 - val_f1_score: 0.8882\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 57s 333ms/step - loss: 0.3618 - acc: 0.8396 - f1_score: 0.8987 - val_loss: 0.4006 - val_acc: 0.8305 - val_f1_score: 0.8910\n",
            "Epoch 5/5\n",
            "172/172 [==============================] - 56s 326ms/step - loss: 0.3842 - acc: 0.8351 - f1_score: 0.8959 - val_loss: 0.4007 - val_acc: 0.8258 - val_f1_score: 0.8917\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee81881550>"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#without dropout + clean text ------------ resturant\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=5, batch_size=50, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#without dropout + clean text ------------ resturant\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=20, batch_size=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcfEG7fs5wAO",
        "outputId": "b50159cf-84f1-46dc-b6da-5631892c9e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "86/86 [==============================] - 50s 581ms/step - loss: 0.1300 - acc: 0.9513 - f1_score: 0.9679 - val_loss: 0.5318 - val_acc: 0.8431 - val_f1_score: 0.8984\n",
            "Epoch 2/20\n",
            "86/86 [==============================] - 46s 531ms/step - loss: 0.1104 - acc: 0.9594 - f1_score: 0.9732 - val_loss: 0.5912 - val_acc: 0.8267 - val_f1_score: 0.8846\n",
            "Epoch 3/20\n",
            "86/86 [==============================] - 44s 517ms/step - loss: 0.0999 - acc: 0.9616 - f1_score: 0.9745 - val_loss: 0.6988 - val_acc: 0.8473 - val_f1_score: 0.9028\n",
            "Epoch 4/20\n",
            "86/86 [==============================] - 46s 538ms/step - loss: 0.0940 - acc: 0.9649 - f1_score: 0.9768 - val_loss: 0.6057 - val_acc: 0.8417 - val_f1_score: 0.8976\n",
            "Epoch 5/20\n",
            "86/86 [==============================] - 46s 540ms/step - loss: 0.4939 - acc: 0.9150 - f1_score: 0.9445 - val_loss: 4.5749 - val_acc: 0.7482 - val_f1_score: 0.8530\n",
            "Epoch 6/20\n",
            "86/86 [==============================] - 46s 537ms/step - loss: 7269897.0000 - acc: 0.7025 - f1_score: 0.8178 - val_loss: 0.7562 - val_acc: 0.6726 - val_f1_score: 0.7938\n",
            "Epoch 7/20\n",
            "86/86 [==============================] - 46s 533ms/step - loss: 0.6467 - acc: 0.7069 - f1_score: 0.8181 - val_loss: 0.6278 - val_acc: 0.7081 - val_f1_score: 0.8206\n",
            "Epoch 8/20\n",
            "86/86 [==============================] - 46s 539ms/step - loss: 0.5779 - acc: 0.7318 - f1_score: 0.8355 - val_loss: 0.6055 - val_acc: 0.7268 - val_f1_score: 0.8334\n",
            "Epoch 9/20\n",
            "86/86 [==============================] - 46s 539ms/step - loss: 0.5628 - acc: 0.7363 - f1_score: 0.8386 - val_loss: 0.5987 - val_acc: 0.7263 - val_f1_score: 0.8338\n",
            "Epoch 10/20\n",
            "86/86 [==============================] - 46s 538ms/step - loss: 0.5559 - acc: 0.7404 - f1_score: 0.8417 - val_loss: 0.5949 - val_acc: 0.7258 - val_f1_score: 0.8330\n",
            "Epoch 11/20\n",
            "86/86 [==============================] - 46s 532ms/step - loss: 0.5516 - acc: 0.7415 - f1_score: 0.8420 - val_loss: 0.5928 - val_acc: 0.7198 - val_f1_score: 0.8281\n",
            "Epoch 12/20\n",
            "86/86 [==============================] - 46s 532ms/step - loss: 0.5484 - acc: 0.7429 - f1_score: 0.8427 - val_loss: 0.5909 - val_acc: 0.7277 - val_f1_score: 0.8344\n",
            "Epoch 13/20\n",
            "86/86 [==============================] - 45s 529ms/step - loss: 0.5457 - acc: 0.7446 - f1_score: 0.8440 - val_loss: 0.5897 - val_acc: 0.7282 - val_f1_score: 0.8346\n",
            "Epoch 14/20\n",
            "86/86 [==============================] - 46s 531ms/step - loss: 0.5438 - acc: 0.7442 - f1_score: 0.8435 - val_loss: 0.5890 - val_acc: 0.7310 - val_f1_score: 0.8367\n",
            "Epoch 15/20\n",
            "86/86 [==============================] - 46s 530ms/step - loss: 0.5427 - acc: 0.7458 - f1_score: 0.8445 - val_loss: 0.5883 - val_acc: 0.7221 - val_f1_score: 0.8293\n",
            "Epoch 16/20\n",
            "86/86 [==============================] - 46s 536ms/step - loss: 0.5414 - acc: 0.7453 - f1_score: 0.8436 - val_loss: 0.5884 - val_acc: 0.7291 - val_f1_score: 0.8360\n",
            "Epoch 17/20\n",
            "86/86 [==============================] - 46s 534ms/step - loss: 0.5405 - acc: 0.7447 - f1_score: 0.8435 - val_loss: 0.5875 - val_acc: 0.7277 - val_f1_score: 0.8344\n",
            "Epoch 18/20\n",
            "86/86 [==============================] - 46s 539ms/step - loss: 0.5396 - acc: 0.7465 - f1_score: 0.8444 - val_loss: 0.5871 - val_acc: 0.7268 - val_f1_score: 0.8334\n",
            "Epoch 19/20\n",
            "86/86 [==============================] - 46s 539ms/step - loss: 0.5386 - acc: 0.7465 - f1_score: 0.8442 - val_loss: 0.5871 - val_acc: 0.7263 - val_f1_score: 0.8334\n",
            "Epoch 20/20\n",
            "86/86 [==============================] - 46s 533ms/step - loss: 0.5380 - acc: 0.7450 - f1_score: 0.8431 - val_loss: 0.5867 - val_acc: 0.7263 - val_f1_score: 0.8330\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee9236ea50>"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsFE57D0X4Oe",
        "outputId": "91382d19-45f3-4537-d74b-0a49f8368435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "172/172 [==============================] - 143s 815ms/step - loss: 299186.5000 - acc: 0.7345 - f1_score: 0.8401 - val_loss: 0.5122 - val_acc: 0.7520 - val_f1_score: 0.8577\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 59s 343ms/step - loss: 0.4972 - acc: 0.7526 - f1_score: 0.8574 - val_loss: 0.5033 - val_acc: 0.7525 - val_f1_score: 0.8571\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 59s 341ms/step - loss: 0.4880 - acc: 0.7574 - f1_score: 0.8595 - val_loss: 0.5001 - val_acc: 0.7506 - val_f1_score: 0.8557\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 56s 326ms/step - loss: 0.4828 - acc: 0.7575 - f1_score: 0.8590 - val_loss: 0.4971 - val_acc: 0.7506 - val_f1_score: 0.8535\n",
            "Epoch 5/5\n",
            "172/172 [==============================] - 58s 340ms/step - loss: 0.4784 - acc: 0.7621 - f1_score: 0.8607 - val_loss: 0.4950 - val_acc: 0.7515 - val_f1_score: 0.8540\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee80e20810>"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#without dropout + clean text + added dense layer ------------ resturant\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=5, batch_size=50, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#without dropout + clean str (few preprocess than clean text) ------------ resturant\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=10, batch_size=50, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCE4jUICCOhE",
        "outputId": "fa81965b-6087-4cc1-ceef-8e1e9a7eb366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 56s 302ms/step - loss: 23.0487 - acc: 0.7456 - f1_score: 0.8482 - val_loss: 0.5044 - val_acc: 0.7501 - val_f1_score: 0.8572\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 53s 308ms/step - loss: 0.7080 - acc: 0.7658 - f1_score: 0.8641 - val_loss: 0.4766 - val_acc: 0.7679 - val_f1_score: 0.8645\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 51s 297ms/step - loss: 0.4507 - acc: 0.7814 - f1_score: 0.8703 - val_loss: 0.4590 - val_acc: 0.7837 - val_f1_score: 0.8695\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 53s 311ms/step - loss: 0.4346 - acc: 0.7936 - f1_score: 0.8752 - val_loss: 0.4452 - val_acc: 0.7940 - val_f1_score: 0.8748\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 51s 298ms/step - loss: 2449.4436 - acc: 0.7958 - f1_score: 0.8752 - val_loss: 0.4362 - val_acc: 0.8010 - val_f1_score: 0.8780\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 53s 308ms/step - loss: 0.4656 - acc: 0.7806 - f1_score: 0.8702 - val_loss: 0.4628 - val_acc: 0.7753 - val_f1_score: 0.8685\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 52s 300ms/step - loss: 0.4438 - acc: 0.7802 - f1_score: 0.8714 - val_loss: 0.4564 - val_acc: 0.7763 - val_f1_score: 0.8687\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 52s 300ms/step - loss: 0.4367 - acc: 0.7858 - f1_score: 0.8735 - val_loss: 0.4517 - val_acc: 0.7833 - val_f1_score: 0.8708\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 54s 313ms/step - loss: 0.4304 - acc: 0.7922 - f1_score: 0.8758 - val_loss: 0.4475 - val_acc: 0.7837 - val_f1_score: 0.8709\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 51s 298ms/step - loss: 0.4244 - acc: 0.7999 - f1_score: 0.8790 - val_loss: 0.4438 - val_acc: 0.7875 - val_f1_score: 0.8728\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e6e944450>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "\n",
        "#------------------------------------------------------ Choose a dataset df\n",
        "#df= res_df\n",
        "df= prod_df\n",
        "#df= htl_df\n",
        "#df= mov_df\n",
        "\n",
        "#################################### vectorizing cleaned text\n",
        "# fit tokenizer vocab \n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.clean_text)\n",
        "\n",
        "################################### standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "\n",
        "################################## indexing + padding\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "print('one training text after padding: ')\n",
        "print(train_text[10])\n",
        "\n",
        "################################### preparing embedding matrix\n",
        "embedding_dim = w2v.vector_size # w2v embedding dim\n",
        "word_index = tokenizer.word_index # vocab\n",
        "\n",
        "# use the gensim model to build a numpy array of embeddings,\n",
        "# we'll feed this array to the keras embeddings layer.\n",
        "# each row i of the array will correspond to the word token assigned to that value \n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = w2v[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except: # word in our data vocab is missing in w2v, will use 0 vector for that word\n",
        "        pass\n",
        "print('embedding matrix shape (vocab, embeddings dim) is ', embedding_matrix.shape)\n",
        "\n",
        "\n",
        "##################################### defining model\n",
        "\n",
        "inp = Input(shape=(seq_len,))\n",
        "x = Embedding(len(word_index) + 1,\n",
        "              embedding_dim,\n",
        "              weights=[embedding_matrix], # where we feed the pretrained vecs\n",
        "              trainable=False)(inp) # freeze these parameters in the model\n",
        "#recurrent_dropout=.1\n",
        "x = Bidirectional(LSTM(64, activation='relu'))(x)\n",
        "x = Dense(32)(x) # fully connected layer on top of the output of the bi-LSTM\n",
        "#x = Dropout(.3)(x)\n",
        "#x = Dense(20)(x) #added\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "\n",
        "print(NN.summary())\n",
        "\n",
        "######################################## compiling model\n",
        "threshold = 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "\n",
        "\n",
        "######################################## fitting model = training\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=10, batch_size=50, verbose=1)\n",
        "\n",
        "\n",
        "######################################### testing\n",
        "\n",
        "# This can be used if we have train, val, test sets\n",
        "# but since data size is not to large, I choose 2 splits only\n",
        "#result = the validation of last epoch\n",
        "\n",
        "prob= NN.predict(val_text)\n",
        "predections= [1 if p >= 0.5 else 0 for p in prob]\n",
        "print('---------------------------RESULT------------------------')\n",
        "print('The model f1 score on validation set is :', f1_score(y_val, predections))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcBAJvA6lALd",
        "outputId": "70e2b378-cc02-4aaa-d577-8b45991bbe7c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one training text after padding: \n",
            "[   5 4911 4912    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "embedding matrix shape (vocab, embeddings dim) is  (9908, 300)\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 128, 300)          2972400   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              186880    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,163,441\n",
            "Trainable params: 191,041\n",
            "Non-trainable params: 2,972,400\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "64/64 [==============================] - 49s 722ms/step - loss: 0.4834 - acc: 0.7711 - f1_score: 0.8698 - val_loss: 0.4247 - val_acc: 0.7932 - val_f1_score: 0.8832\n",
            "Epoch 2/10\n",
            "64/64 [==============================] - 46s 715ms/step - loss: 0.5175 - acc: 0.8196 - f1_score: 0.8920 - val_loss: 0.3971 - val_acc: 0.8134 - val_f1_score: 0.8932\n",
            "Epoch 3/10\n",
            "64/64 [==============================] - 46s 720ms/step - loss: 1.6917 - acc: 0.8171 - f1_score: 0.8901 - val_loss: 0.4207 - val_acc: 0.8235 - val_f1_score: 0.8971\n",
            "Epoch 4/10\n",
            "64/64 [==============================] - 46s 716ms/step - loss: 0.3806 - acc: 0.8414 - f1_score: 0.9065 - val_loss: 0.3752 - val_acc: 0.8398 - val_f1_score: 0.9044\n",
            "Epoch 5/10\n",
            "64/64 [==============================] - 45s 701ms/step - loss: 0.3427 - acc: 0.8562 - f1_score: 0.9138 - val_loss: 0.3766 - val_acc: 0.8386 - val_f1_score: 0.9033\n",
            "Epoch 6/10\n",
            "64/64 [==============================] - 45s 709ms/step - loss: 0.3251 - acc: 0.8682 - f1_score: 0.9201 - val_loss: 0.3688 - val_acc: 0.8499 - val_f1_score: 0.9095\n",
            "Epoch 7/10\n",
            "64/64 [==============================] - 45s 711ms/step - loss: 0.3126 - acc: 0.8726 - f1_score: 0.9225 - val_loss: 0.3893 - val_acc: 0.8436 - val_f1_score: 0.9024\n",
            "Epoch 8/10\n",
            "64/64 [==============================] - 45s 696ms/step - loss: 0.3094 - acc: 0.8742 - f1_score: 0.9234 - val_loss: 0.3724 - val_acc: 0.8525 - val_f1_score: 0.9082\n",
            "Epoch 9/10\n",
            "64/64 [==============================] - 45s 702ms/step - loss: 0.2903 - acc: 0.8821 - f1_score: 0.9280 - val_loss: 0.3769 - val_acc: 0.8525 - val_f1_score: 0.9081\n",
            "Epoch 10/10\n",
            "64/64 [==============================] - 45s 702ms/step - loss: 0.2689 - acc: 0.8934 - f1_score: 0.9347 - val_loss: 0.3820 - val_acc: 0.8499 - val_f1_score: 0.9077\n",
            "---------------------------RESULT------------------------\n",
            "The model f1 score on validation set is : 0.9076803723816913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "\n",
        "#------------------------------------------------------ Choose a dataset df\n",
        "#df= res_df\n",
        "#df= prod_df\n",
        "df= htl_df\n",
        "#df= mov_df\n",
        "\n",
        "#################################### vectorizing cleaned text\n",
        "# fit tokenizer vocab \n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.clean_text)\n",
        "\n",
        "################################### standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "\n",
        "################################## indexing + padding\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "print('one training text after padding: ')\n",
        "print(train_text[10])\n",
        "\n",
        "################################### preparing embedding matrix\n",
        "embedding_dim = w2v.vector_size # w2v embedding dim\n",
        "word_index = tokenizer.word_index # vocab\n",
        "\n",
        "# use the gensim model to build a numpy array of embeddings,\n",
        "# we'll feed this array to the keras embeddings layer.\n",
        "# each row i of the array will correspond to the word token assigned to that value \n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = w2v[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except: # word in our data vocab is missing in w2v, will use 0 vector for that word\n",
        "        pass\n",
        "print('embedding matrix shape (vocab, embeddings dim) is ', embedding_matrix.shape)\n",
        "\n",
        "\n",
        "##################################### defining model\n",
        "\n",
        "inp = Input(shape=(seq_len,))\n",
        "x = Embedding(len(word_index) + 1,\n",
        "              embedding_dim,\n",
        "              weights=[embedding_matrix], # where we feed the pretrained vecs\n",
        "              trainable=False)(inp) # freeze these parameters in the model\n",
        "#recurrent_dropout=.1\n",
        "x = Bidirectional(LSTM(64, activation='relu'))(x)\n",
        "x = Dense(32)(x) # fully connected layer on top of the output of the bi-LSTM\n",
        "#x = Dropout(.3)(x)\n",
        "#x = Dense(20)(x) #added\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "\n",
        "print(NN.summary())\n",
        "\n",
        "######################################## compiling model\n",
        "threshold = 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "\n",
        "\n",
        "######################################## fitting model = training\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=10, batch_size=50, verbose=1)\n",
        "\n",
        "\n",
        "######################################### testing\n",
        "\n",
        "# This can be used if we have train, val, test sets\n",
        "# but since data size is not to large, I choose 2 splits only\n",
        "#result = the validation of last epoch\n",
        "\n",
        "prob= NN.predict(val_text)\n",
        "predections= [1 if p >= 0.5 else 0 for p in prob]\n",
        "print('---------------------------RESULT------------------------')\n",
        "print('The model f1 score on validation set is :', f1_score(y_val, predections))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8JhVDthp5u7",
        "outputId": "ad06cd8a-67b8-43d4-cf61-04d176e077a7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one training text after padding: \n",
            "[73645   669    65 16068 24665   354  1963 25444  9015   681 37482 37483\n",
            " 14973  2873  2968    11    47  2264  1432  5626   438   782     7 17589\n",
            " 12493  5977 15135  2909 12146   920   258    78   979 20174   776   125\n",
            "    12   696   942  4167    54     1 13347    82   235  1445   391  1574\n",
            "   415   301     1   208   802   965    45 73646    60  3204   871   728\n",
            " 34468 19787   432     8   924  1264 73647     1  2172  7351 21030 16417\n",
            " 22121  7327  3070  6356  5940    10  2035   691  5166   162    29 26612\n",
            "   180  3493  4637   348    24   582  2714  3006 73648 16353  1101 73649\n",
            "   188   166  1459   162   288  2536   620     3  1442   976    34   385\n",
            "  5595     7  5626  2264  6539 26347    13   813 31132   317     1  1631\n",
            " 13556   586   592 36829 12184  3822    19    23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding matrix shape (vocab, embeddings dim) is  (85463, 300)\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 128, 300)          25638900  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 128)              186880    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,829,941\n",
            "Trainable params: 191,041\n",
            "Non-trainable params: 25,638,900\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "215/215 [==============================] - 164s 734ms/step - loss: 9315876.0000 - acc: 0.8197 - f1_score: 0.8980 - val_loss: 0.3660 - val_acc: 0.8358 - val_f1_score: 0.9065\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 154s 715ms/step - loss: 9320869.0000 - acc: 0.8382 - f1_score: 0.9067 - val_loss: 7.5211 - val_acc: 0.8540 - val_f1_score: 0.9146\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 153s 713ms/step - loss: 0.3454 - acc: 0.8500 - f1_score: 0.9134 - val_loss: 0.3322 - val_acc: 0.8618 - val_f1_score: 0.9194\n",
            "Epoch 4/10\n",
            "215/215 [==============================] - 179s 832ms/step - loss: 0.4569 - acc: 0.8542 - f1_score: 0.9148 - val_loss: 0.3250 - val_acc: 0.8600 - val_f1_score: 0.9178\n",
            "Epoch 5/10\n",
            "215/215 [==============================] - 187s 865ms/step - loss: 0.3574 - acc: 0.8568 - f1_score: 0.9157 - val_loss: 0.3196 - val_acc: 0.8622 - val_f1_score: 0.9187\n",
            "Epoch 6/10\n",
            "215/215 [==============================] - 200s 933ms/step - loss: 0.3449 - acc: 0.8587 - f1_score: 0.9166 - val_loss: 0.3161 - val_acc: 0.8607 - val_f1_score: 0.9178\n",
            "Epoch 7/10\n",
            "215/215 [==============================] - 248s 1s/step - loss: 0.3116 - acc: 0.8611 - f1_score: 0.9177 - val_loss: 0.3135 - val_acc: 0.8652 - val_f1_score: 0.9199\n",
            "Epoch 8/10\n",
            "215/215 [==============================] - 187s 869ms/step - loss: 0.3088 - acc: 0.8631 - f1_score: 0.9187 - val_loss: 0.3108 - val_acc: 0.8641 - val_f1_score: 0.9197\n",
            "Epoch 9/10\n",
            "215/215 [==============================] - 186s 868ms/step - loss: 0.3065 - acc: 0.8634 - f1_score: 0.9188 - val_loss: 0.3087 - val_acc: 0.8670 - val_f1_score: 0.9208\n",
            "Epoch 10/10\n",
            "215/215 [==============================] - 152s 708ms/step - loss: 0.3046 - acc: 0.8656 - f1_score: 0.9200 - val_loss: 0.3069 - val_acc: 0.8689 - val_f1_score: 0.9217\n",
            "---------------------------RESULT------------------------\n",
            "The model f1 score on validation set is : 0.9217429968875055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "\n",
        "#------------------------------------------------------ Choose a dataset df\n",
        "#df= res_df\n",
        "#df= prod_df\n",
        "#df= htl_df\n",
        "df= mov_df\n",
        "\n",
        "#################################### vectorizing cleaned text\n",
        "# fit tokenizer vocab \n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.clean_text)\n",
        "\n",
        "################################### standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "\n",
        "################################## indexing + padding\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "print('one training text after padding: ')\n",
        "print(train_text[10])\n",
        "\n",
        "################################### preparing embedding matrix\n",
        "embedding_dim = w2v.vector_size # w2v embedding dim\n",
        "word_index = tokenizer.word_index # vocab\n",
        "\n",
        "# use the gensim model to build a numpy array of embeddings,\n",
        "# we'll feed this array to the keras embeddings layer.\n",
        "# each row i of the array will correspond to the word token assigned to that value \n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = w2v[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except: # word in our data vocab is missing in w2v, will use 0 vector for that word\n",
        "        pass\n",
        "print('embedding matrix shape (vocab, embeddings dim) is ', embedding_matrix.shape)\n",
        "\n",
        "\n",
        "##################################### defining model\n",
        "\n",
        "inp = Input(shape=(seq_len,))\n",
        "x = Embedding(len(word_index) + 1,\n",
        "              embedding_dim,\n",
        "              weights=[embedding_matrix], # where we feed the pretrained vecs\n",
        "              trainable=False)(inp) # freeze these parameters in the model\n",
        "#recurrent_dropout=.1\n",
        "x = Bidirectional(LSTM(64, activation='relu'))(x)\n",
        "x = Dense(32)(x) # fully connected layer on top of the output of the bi-LSTM\n",
        "#x = Dropout(.3)(x)\n",
        "#x = Dense(20)(x) #added\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "\n",
        "print(NN.summary())\n",
        "\n",
        "######################################## compiling model\n",
        "threshold = 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "\n",
        "\n",
        "######################################## fitting model = training\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=10, batch_size=50, verbose=1)\n",
        "\n",
        "\n",
        "######################################### testing\n",
        "\n",
        "# This can be used if we have train, val, test sets\n",
        "# but since data size is not to large, I choose 2 splits only\n",
        "#result = the validation of last epoch\n",
        "\n",
        "prob= NN.predict(val_text)\n",
        "predections= [1 if p >= 0.5 else 0 for p in prob]\n",
        "print('---------------------------RESULT------------------------')\n",
        "print('The model f1 score on validation set is :', f1_score(y_val, predections))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsdfvkYBsVi4",
        "outputId": "6aae4c62-530a-4be1-e423-a96def3734aa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one training text after padding: \n",
            "[ 5578  7055   263  6755   125   890    71  6265    22  6799  1160 29361\n",
            "   603    79    49  1314    53   327  2713   403  2690  2415    15  1230\n",
            " 29362    89   673 14053  4009  2796  2090 13427 14054  2090  3698 14055\n",
            " 18734   398   731   689  1668   164  2351    58 18735     7  3719    32\n",
            "   100  6806  1990  2791 29363    18    32  1277 29364 29365 18736  1045\n",
            "    58  5629 29366  1912     7    42   204   284 11267 18737 14056 14057\n",
            " 11268  6266    22  2284   329 18643  5130  7971   982    40    22  9028\n",
            "   303   505    49  4674    81 29367  3475 29368   372     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding matrix shape (vocab, embeddings dim) is  (62508, 300)\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, 128, 300)          18752400  \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 128)              186880    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,943,441\n",
            "Trainable params: 191,041\n",
            "Non-trainable params: 18,752,400\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "22/22 [==============================] - 22s 746ms/step - loss: 0.6031 - acc: 0.7098 - f1_score: 0.8293 - val_loss: 0.5899 - val_acc: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 16s 709ms/step - loss: 0.5665 - acc: 0.7163 - f1_score: 0.8347 - val_loss: 0.5670 - val_acc: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 16s 724ms/step - loss: 1728909824.0000 - acc: 0.7246 - f1_score: 0.8382 - val_loss: 35335249920.0000 - val_acc: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 24s 1s/step - loss: 31644477440.0000 - acc: 0.6118 - f1_score: 0.7420 - val_loss: 7266472960.0000 - val_acc: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 16s 725ms/step - loss: 2369958912.0000 - acc: 0.5961 - f1_score: 0.7222 - val_loss: 1956896128.0000 - val_acc: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 16s 725ms/step - loss: 885432768.0000 - acc: 0.5462 - f1_score: 0.6702 - val_loss: 3275115264.0000 - val_acc: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 16s 714ms/step - loss: 2065573120.0000 - acc: 0.6248 - f1_score: 0.7580 - val_loss: 430546112.0000 - val_acc: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 16s 718ms/step - loss: 748094656.0000 - acc: 0.5924 - f1_score: 0.7168 - val_loss: 416185024.0000 - val_acc: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 16s 713ms/step - loss: 262005696.0000 - acc: 0.5860 - f1_score: 0.7125 - val_loss: 344072224.0000 - val_acc: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 16s 746ms/step - loss: 213654864.0000 - acc: 0.6137 - f1_score: 0.7293 - val_loss: 369937696.0000 - val_acc: 0.7159 - val_f1_score: 0.8344\n",
            "---------------------------RESULT------------------------\n",
            "The model f1 score on validation set is : 0.8344086021505376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "\n",
        "#------------------------------------------------------ Choose a dataset df\n",
        "#df= res_df\n",
        "#df= prod_df\n",
        "#df= htl_df\n",
        "#df= mov_df\n",
        "\n",
        "df= pd.concat([res_df,prod_df,htl_df,mov_df])\n",
        "\n",
        "#################################### vectorizing cleaned text\n",
        "# fit tokenizer vocab \n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.clean_text)\n",
        "\n",
        "################################### standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "\n",
        "################################## indexing + padding\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "print('one training text after padding: ')\n",
        "print(train_text[10])\n",
        "\n",
        "################################### preparing embedding matrix\n",
        "embedding_dim = w2v.vector_size # w2v embedding dim\n",
        "word_index = tokenizer.word_index # vocab\n",
        "\n",
        "# use the gensim model to build a numpy array of embeddings,\n",
        "# we'll feed this array to the keras embeddings layer.\n",
        "# each row i of the array will correspond to the word token assigned to that value \n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = w2v[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except: # word in our data vocab is missing in w2v, will use 0 vector for that word\n",
        "        pass\n",
        "print('embedding matrix shape (vocab, embeddings dim) is ', embedding_matrix.shape)\n",
        "\n",
        "\n",
        "##################################### defining model\n",
        "\n",
        "inp = Input(shape=(seq_len,))\n",
        "x = Embedding(len(word_index) + 1,\n",
        "              embedding_dim,\n",
        "              weights=[embedding_matrix], # where we feed the pretrained vecs\n",
        "              trainable=False)(inp) # freeze these parameters in the model\n",
        "#recurrent_dropout=.1\n",
        "x = Bidirectional(LSTM(64, activation='relu'))(x)\n",
        "x = Dense(32)(x) # fully connected layer on top of the output of the bi-LSTM\n",
        "#x = Dropout(.3)(x)\n",
        "#x = Dense(20)(x) #added\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "\n",
        "print(NN.summary())\n",
        "\n",
        "######################################## compiling model\n",
        "threshold = 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "\n",
        "\n",
        "######################################## fitting model = training\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=10, batch_size=50, verbose=1)\n",
        "\n",
        "\n",
        "######################################### testing\n",
        "\n",
        "# This can be used if we have train, val, test sets\n",
        "# but since data size is not to large, I choose 2 splits only\n",
        "#result = the validation of last epoch\n",
        "\n",
        "prob= NN.predict(val_text)\n",
        "predections= [1 if p >= 0.5 else 0 for p in prob]\n",
        "print('---------------------------RESULT------------------------')\n",
        "print('The model f1 score on validation set is :', f1_score(y_val, predections))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVa4p2xVzgNs",
        "outputId": "dfd44c43-57ad-4096-ff3d-47dfee5bc692"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one training text after padding: \n",
            "[    91   4921    961   2039    227    224    353  17643  44581     91\n",
            "    401   2813     85      6  60782    220    131   6108     11    241\n",
            "    371    467     51    614   3501   2989     55     78    182   1699\n",
            "     36      6  35705 114340   2042   9754   9779  20073      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding matrix shape (vocab, embeddings dim) is  (147742, 300)\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, 128, 300)          44322600  \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 128)              186880    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44,513,641\n",
            "Trainable params: 191,041\n",
            "Non-trainable params: 44,322,600\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "472/472 [==============================] - 355s 747ms/step - loss: 1861916688384.0000 - acc: 0.6684 - f1_score: 0.7887 - val_loss: 98822160.0000 - val_acc: 0.4974 - val_f1_score: 0.6048\n",
            "Epoch 2/10\n",
            "472/472 [==============================] - 336s 711ms/step - loss: 655627456.0000 - acc: 0.6462 - f1_score: 0.7715 - val_loss: 51939732.0000 - val_acc: 0.7575 - val_f1_score: 0.8596\n",
            "Epoch 3/10\n",
            "472/472 [==============================] - 336s 712ms/step - loss: 52564784.0000 - acc: 0.6531 - f1_score: 0.7780 - val_loss: 50942828.0000 - val_acc: 0.4862 - val_f1_score: 0.5925\n",
            "Epoch 4/10\n",
            "472/472 [==============================] - 334s 708ms/step - loss: 51231908.0000 - acc: 0.6543 - f1_score: 0.7784 - val_loss: 49862992.0000 - val_acc: 0.7769 - val_f1_score: 0.8744\n",
            "Epoch 5/10\n",
            "472/472 [==============================] - 328s 694ms/step - loss: 49489592.0000 - acc: 0.6569 - f1_score: 0.7804 - val_loss: 47304004.0000 - val_acc: 0.7596 - val_f1_score: 0.8611\n",
            "Epoch 6/10\n",
            "472/472 [==============================] - 339s 718ms/step - loss: 47388952.0000 - acc: 0.6549 - f1_score: 0.7789 - val_loss: 44522456.0000 - val_acc: 0.7769 - val_f1_score: 0.8744\n",
            "Epoch 7/10\n",
            "472/472 [==============================] - 446s 943ms/step - loss: 44760188.0000 - acc: 0.6545 - f1_score: 0.7786 - val_loss: 42020980.0000 - val_acc: 0.4873 - val_f1_score: 0.5933\n",
            "Epoch 8/10\n",
            "472/472 [==============================] - 500s 1s/step - loss: 41098928.0000 - acc: 0.6546 - f1_score: 0.7789 - val_loss: 37765356.0000 - val_acc: 0.7770 - val_f1_score: 0.8745\n",
            "Epoch 9/10\n",
            "472/472 [==============================] - 425s 900ms/step - loss: 36676448.0000 - acc: 0.6529 - f1_score: 0.7776 - val_loss: 33077238.0000 - val_acc: 0.5138 - val_f1_score: 0.6354\n",
            "Epoch 10/10\n",
            "472/472 [==============================] - 341s 722ms/step - loss: 31229610.0000 - acc: 0.6541 - f1_score: 0.7782 - val_loss: 28954276.0000 - val_acc: 0.4870 - val_f1_score: 0.5931\n",
            "---------------------------RESULT------------------------\n",
            "The model f1 score on validation set is : 0.5931313131313131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ArSenTD-Lev (Arabic Sentiment Twitter Dataset for LEVantine dialect)**"
      ],
      "metadata": {
        "id": "E3-H1omk-DM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this dataset, I will use w2v pretrained on Twitter tweets.\n",
        "Since the dataset is small, I will not consider single domains."
      ],
      "metadata": {
        "id": "Cp5f5SW0_39B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://bakrianoo.ewr1.vultrobjects.com/aravec/full_uni_sg_300_twitter.zip'\n",
        "!unzip 'full_uni_sg_300_twitter.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4WELvi2-4f-",
        "outputId": "4617216a-0332-4cbf-f085-5eed09fbeab4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-15 17:00:51--  https://bakrianoo.ewr1.vultrobjects.com/aravec/full_uni_sg_300_twitter.zip\n",
            "Resolving bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)... 108.61.0.122, 2001:19f0:0:22::100\n",
            "Connecting to bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)|108.61.0.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2832094149 (2.6G) [application/zip]\n",
            "Saving to: ‘full_uni_sg_300_twitter.zip’\n",
            "\n",
            "full_uni_sg_300_twi 100%[===================>]   2.64G  47.8MB/s    in 38s     \n",
            "\n",
            "2021-12-15 17:01:29 (71.2 MB/s) - ‘full_uni_sg_300_twitter.zip’ saved [2832094149/2832094149]\n",
            "\n",
            "Archive:  full_uni_sg_300_twitter.zip\n",
            "  inflating: full_uni_sg_300_twitter.mdl  \n",
            "  inflating: full_uni_sg_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_sg_300_twitter.mdl.wv.vectors.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the AraVec model\n",
        "tw2v = gensim.models.Word2Vec.load(\"full_uni_sg_300_twitter.mdl\")\n",
        "print(\"We've\",len(tw2v.wv.index2word),\"vocabularies\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNddBqDI-olV",
        "outputId": "7b90d403-ce23-4ae4-8e6c-5a09311cc452"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We've 1259756 vocabularies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                                   u\"\\U00002702-\\U000027B0\"\n",
        "                                   u\"\\U000024C2-\\U0001F251\"\n",
        "                                   \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "4qnR_v5N7i2s"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define 'clean_tweet' function to clean the text and remove unwanted text parts\n",
        "def clean_tweet(text):\n",
        "    # define regular expression patterns\n",
        "    p_english = \"[a-zA-Z0-9]+\"\n",
        "    p_url = \"https?://[A-Za-z0-9./]+\"\n",
        "    p_mention = \"\\@[\\_0-9a-zA-Z]+\\:?\"    \n",
        "    p_retweet = \"RT \\@[\\_\\-0-9a-zA-Z]+\\:?\"\n",
        "    p_punctuations = \"[\" + string.punctuation + \"]\"\n",
        "    \n",
        "    # remove unwanted parts\n",
        "    text = re.sub(p_english, ' ', text)\n",
        "    text = re.sub(p_retweet, ' ', text)\n",
        "    text = re.sub(p_mention, ' ', text)\n",
        "    text = re.sub(p_url, ' ', text)\n",
        "    text = re.sub(p_punctuations, ' ', text)\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    \n",
        "    # remove الهمزة\n",
        "    text = re.sub(\"[أإآ]\", 'ا', text)\n",
        "    text = re.sub(\"ة\", 'ه', text)\n",
        "    text = re.sub(\"ى\", 'ي', text)\n",
        "    \n",
        "    # removing tashkeel\n",
        "    tashkel = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(tashkel, '', text)\n",
        "    \n",
        "    # remove repeated letters more than two letters\n",
        "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    text= remove_emoji(text)\n",
        "    \n",
        "    # remove stopwords\n",
        "    words = [word for word in text.split() if word not in ar_stopwords]\n",
        "    words = [word for word in words if len(word)>=2]\n",
        "    \n",
        "    # merge and return final text\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "KoHHfAHG8-Oy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read files of first dataset (MSA)\n",
        "cor_df=pd.read_csv('ArSenTD-LEV.tsv', sep='\\t')\n",
        "cor_df= cor_df.drop(columns=['Country', 'Sentiment_Expression','Sentiment_Target'])\n",
        "cor_df=cor_df[cor_df['Sentiment']!='neutral'].reset_index(drop=True)\n",
        "#unify format with other dataset\n",
        "cor_df['Sentiment'].replace({'very_positive': '1', 'positive': '1', 'very_negative':'0', 'negative':'0'}, inplace=True)\n",
        "cor_df= cor_df.reset_index(drop=True)\n",
        "cor_df.rename(columns={'Tweet': 'text', 'Sentiment': 'polarity'}, inplace=True)\n",
        "cor_df[\"polarity\"] = pd.to_numeric(cor_df[\"polarity\"])\n",
        "cor_df['clean_text'] = cor_df['text'].apply(clean_tweet)\n",
        "cor_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ewhjR1SK75nu",
        "outputId": "a0fe1aab-8710-4f85-ae8a-19d4ee709190"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Topic</th>\n",
              "      <th>polarity</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"أنا أؤمن بأن الانسان ينطفئ جماله عند ابتعاد م...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "      <td>اؤمن بان الانسان ينطفئ جماله ابتعاد يحب بريق ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>من الذاكره... @3FInQe . عندما اعتقد كريستيانو ...</td>\n",
              "      <td>sports</td>\n",
              "      <td>1</td>\n",
              "      <td>الذاكره اعتقد كريستيانو افضل لاعب العالم كاكا ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#مصطلحات_لبنانيه_حيرت_البشريه بتوصل عالبيت ، ب...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "      <td>مصطلحات لبنانيه حيرت البشريه بتوصل عالبيت بنط ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>نصمت !! لتسير حياتنا على مً يرام فالناّس لم تع...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "      <td>نصمت لتسير حياتنا يرام فالناس تعد نقيه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Yousef_MUFC اكثر ما يزعجنا بعد مستوانا خارج ا...</td>\n",
              "      <td>sports</td>\n",
              "      <td>0</td>\n",
              "      <td>يزعجنا مستوانا خارج ارضنا تمثيل كونتي فوزه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3110</th>\n",
              "      <td>نهتم من خلال خدمة تنسيق الرسائل بإظهار رسالة ا...</td>\n",
              "      <td>education</td>\n",
              "      <td>1</td>\n",
              "      <td>نهتم خدمه تنسيق الرسائل باظهار رساله الماجستير...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3111</th>\n",
              "      <td>صلاح من لاعب في المقاولون العرب يحلم ان يلعب ل...</td>\n",
              "      <td>sports</td>\n",
              "      <td>1</td>\n",
              "      <td>صلاح لاعب المقاولون العرب يحلم يلعب للاهلي وال...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3112</th>\n",
              "      <td>الملك سلمان بن عبد العزيز: تطبيق الأنظمة بحزم ...</td>\n",
              "      <td>politics</td>\n",
              "      <td>1</td>\n",
              "      <td>الملك سلمان عبد العزيز تطبيق الانظمه بحزم تطاو...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3113</th>\n",
              "      <td>@ZahraaIraq9 😂 كل ما ادخل حسابي الكه تغريداتج ...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "      <td>ادخل حسابي الكه تغريداتج حب العراق وانتي هسه ي...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3114</th>\n",
              "      <td>شو هالشعب نحنا اللي عايش بلا مي وكهربا والزبال...</td>\n",
              "      <td>politics</td>\n",
              "      <td>0</td>\n",
              "      <td>شو هالشعب نحنا اللي عايش مي وكهربا والزباله وا...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3115 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ...                                         clean_text\n",
              "0     \"أنا أؤمن بأن الانسان ينطفئ جماله عند ابتعاد م...  ...  اؤمن بان الانسان ينطفئ جماله ابتعاد يحب بريق ا...\n",
              "1     من الذاكره... @3FInQe . عندما اعتقد كريستيانو ...  ...  الذاكره اعتقد كريستيانو افضل لاعب العالم كاكا ...\n",
              "2     #مصطلحات_لبنانيه_حيرت_البشريه بتوصل عالبيت ، ب...  ...  مصطلحات لبنانيه حيرت البشريه بتوصل عالبيت بنط ...\n",
              "3     نصمت !! لتسير حياتنا على مً يرام فالناّس لم تع...  ...             نصمت لتسير حياتنا يرام فالناس تعد نقيه\n",
              "4     @Yousef_MUFC اكثر ما يزعجنا بعد مستوانا خارج ا...  ...         يزعجنا مستوانا خارج ارضنا تمثيل كونتي فوزه\n",
              "...                                                 ...  ...                                                ...\n",
              "3110  نهتم من خلال خدمة تنسيق الرسائل بإظهار رسالة ا...  ...  نهتم خدمه تنسيق الرسائل باظهار رساله الماجستير...\n",
              "3111  صلاح من لاعب في المقاولون العرب يحلم ان يلعب ل...  ...  صلاح لاعب المقاولون العرب يحلم يلعب للاهلي وال...\n",
              "3112  الملك سلمان بن عبد العزيز: تطبيق الأنظمة بحزم ...  ...  الملك سلمان عبد العزيز تطبيق الانظمه بحزم تطاو...\n",
              "3113  @ZahraaIraq9 😂 كل ما ادخل حسابي الكه تغريداتج ...  ...  ادخل حسابي الكه تغريداتج حب العراق وانتي هسه ي...\n",
              "3114  شو هالشعب نحنا اللي عايش بلا مي وكهربا والزبال...  ...  شو هالشعب نحنا اللي عايش مي وكهربا والزباله وا...\n",
              "\n",
              "[3115 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "\n",
        "\n",
        "df= cor_df\n",
        "\n",
        "#################################### vectorizing cleaned text\n",
        "# fit tokenizer vocab \n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.clean_text)\n",
        "\n",
        "################################### standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "\n",
        "################################## indexing + padding\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len, padding='post')\n",
        "\n",
        "print('one training text after padding: ')\n",
        "print(train_text[10])\n",
        "\n",
        "################################### preparing embedding matrix\n",
        "embedding_dim = tw2v.vector_size # w2v embedding dim\n",
        "word_index = tokenizer.word_index # vocab\n",
        "\n",
        "# use the gensim model to build a numpy array of embeddings,\n",
        "# we'll feed this array to the keras embeddings layer.\n",
        "# each row i of the array will correspond to the word token assigned to that value \n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = tw2v[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except: # word in our data vocab is missing in w2v, will use 0 vector for that word\n",
        "        pass\n",
        "print('embedding matrix shape (vocab, embeddings dim) is ', embedding_matrix.shape)\n",
        "\n",
        "\n",
        "##################################### defining model\n",
        "\n",
        "inp = Input(shape=(seq_len,))\n",
        "x = Embedding(len(word_index) + 1,\n",
        "              embedding_dim,\n",
        "              weights=[embedding_matrix], # where we feed the pretrained vecs\n",
        "              trainable=False)(inp) # freeze these parameters in the model\n",
        "#recurrent_dropout=.1\n",
        "x = Bidirectional(LSTM(64, activation='relu'))(x)\n",
        "x = Dense(32)(x) # fully connected layer on top of the output of the bi-LSTM\n",
        "#x = Dropout(.3)(x)\n",
        "#x = Dense(20)(x) #added\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "\n",
        "print(NN.summary())\n",
        "\n",
        "######################################## compiling model\n",
        "threshold = 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "\n",
        "\n",
        "######################################## fitting model = training\n",
        "NN.fit(train_text, y_train, validation_data=(val_text, y_val), epochs=10, batch_size=50, verbose=1)\n",
        "\n",
        "\n",
        "######################################### testing\n",
        "\n",
        "# This can be used if we have train, val, test sets\n",
        "# but since data size is not to large, I choose 2 splits only\n",
        "#result = the validation of last epoch\n",
        "\n",
        "prob= NN.predict(val_text)\n",
        "predections= [1 if p >= 0.5 else 0 for p in prob]\n",
        "print('---------------------------RESULT------------------------')\n",
        "print('The model f1 score on validation set is :', f1_score(y_val, predections))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OooAk2my-TL1",
        "outputId": "edd77c76-663b-4396-804c-93f47448e52c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one training text after padding: \n",
            "[   43    43    43    76   412 16479  1436    38 16480 16481 16482 16483\n",
            " 16484  4526 16485 16486   446     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding matrix shape (vocab, embeddings dim) is  (17055, 300)\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     (None, 128, 300)          5116500   \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 128)              186880    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,307,541\n",
            "Trainable params: 191,041\n",
            "Non-trainable params: 5,116,500\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 41s 768ms/step - loss: 0.5644 - acc: 0.7047 - f1_score: 0.5534 - val_loss: 0.4898 - val_acc: 0.7801 - val_f1_score: 0.6821\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 37s 743ms/step - loss: 0.4368 - acc: 0.8146 - f1_score: 0.7489 - val_loss: 0.4138 - val_acc: 0.8202 - val_f1_score: 0.7443\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 37s 743ms/step - loss: 0.3458 - acc: 0.8531 - f1_score: 0.8068 - val_loss: 0.4011 - val_acc: 0.8186 - val_f1_score: 0.7290\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 37s 743ms/step - loss: 0.2827 - acc: 0.8800 - f1_score: 0.8420 - val_loss: 0.3993 - val_acc: 0.8299 - val_f1_score: 0.7985\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 37s 738ms/step - loss: 0.2386 - acc: 0.8989 - f1_score: 0.8692 - val_loss: 0.4255 - val_acc: 0.8523 - val_f1_score: 0.7870\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 37s 737ms/step - loss: 0.1747 - acc: 0.9310 - f1_score: 0.9116 - val_loss: 0.4146 - val_acc: 0.8475 - val_f1_score: 0.7743\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 37s 733ms/step - loss: 0.1225 - acc: 0.9539 - f1_score: 0.9408 - val_loss: 0.4637 - val_acc: 0.8620 - val_f1_score: 0.8037\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 37s 737ms/step - loss: 0.1045 - acc: 0.9623 - f1_score: 0.9520 - val_loss: 0.4070 - val_acc: 0.8668 - val_f1_score: 0.8363\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 38s 759ms/step - loss: 0.0863 - acc: 0.9727 - f1_score: 0.9654 - val_loss: 0.5671 - val_acc: 0.8684 - val_f1_score: 0.8327\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 37s 749ms/step - loss: 0.1539 - acc: 0.9522 - f1_score: 0.9390 - val_loss: 0.5379 - val_acc: 0.8732 - val_f1_score: 0.8225\n",
            "---------------------------RESULT------------------------\n",
            "The model f1 score on validation set is : 0.8224719101123595\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tut.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}