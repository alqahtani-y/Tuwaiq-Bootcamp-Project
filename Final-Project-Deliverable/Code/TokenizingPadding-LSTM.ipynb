{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled21.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APE9Tb6bs6ti"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Arabic-Stopwords\n",
        "!pip install arabic_reshaper\n",
        "!pip install python-bidi\n",
        "!pip install tensorflow-addons #to use f1 score in complie's metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpKBa5P-v3p4",
        "outputId": "300b4469-060e-43bb-dab1-c3a7674ee893"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Arabic-Stopwords\n",
            "  Downloading Arabic_Stopwords-0.3-py3-none-any.whl (353 kB)\n",
            "\u001b[K     |████████████████████████████████| 353 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting pyarabic>=0.6.2\n",
            "  Downloading PyArabic-0.6.14-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 38.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic>=0.6.2->Arabic-Stopwords) (1.15.0)\n",
            "Installing collected packages: pyarabic, Arabic-Stopwords\n",
            "Successfully installed Arabic-Stopwords-0.3 pyarabic-0.6.14\n",
            "Collecting arabic_reshaper\n",
            "  Downloading arabic_reshaper-2.1.3-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from arabic_reshaper) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from arabic_reshaper) (57.4.0)\n",
            "Installing collected packages: arabic-reshaper\n",
            "Successfully installed arabic-reshaper-2.1.3\n",
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-bidi) (1.15.0)\n",
            "Installing collected packages: python-bidi\n",
            "Successfully installed python-bidi-0.4.2\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM\n",
        "import tensorflow_addons as tfa\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "1ZUj92-yv5p7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read files of first dataset (MSA)\n",
        "res_df=pd.read_csv('https://raw.githubusercontent.com/hadyelsahar/large-arabic-sentiment-analysis-resouces/master/datasets/RES.csv')\n",
        "prod_df=pd.read_csv('https://raw.githubusercontent.com/hadyelsahar/large-arabic-sentiment-analysis-resouces/master/datasets/PROD.csv')\n",
        "htl_df=pd.read_csv('https://raw.githubusercontent.com/hadyelsahar/large-arabic-sentiment-analysis-resouces/master/datasets/HTL.csv')\n",
        "mov_df=pd.read_csv('https://raw.githubusercontent.com/hadyelsahar/large-arabic-sentiment-analysis-resouces/master/datasets/MOV.csv')"
      ],
      "metadata": {
        "id": "wjm_T5P0v909"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keep only binary classes (pos & neg)\n",
        "res_df=res_df[res_df['polarity']!=0].reset_index(drop=True)\n",
        "prod_df=prod_df[prod_df['polarity']!=0].reset_index(drop=True)\n",
        "htl_df=htl_df[htl_df['polarity']!=0].reset_index(drop=True)\n",
        "mov_df=mov_df[mov_df['polarity']!=0].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "p6dr0BWMwABf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetDict = {\"resturants\": res_df, \"products\": prod_df, \"hotels\": htl_df, \"movies\": mov_df}"
      ],
      "metadata": {
        "id": "ZMM6gM6uwA2W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert neg label from -1 to 0\n",
        "for k, v in datasetDict.items():\n",
        "  v['polarity'].replace({-1: 0}, inplace=True)"
      ],
      "metadata": {
        "id": "zVfDJdwtwDNi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import string\n",
        "import arabicstopwords.arabicstopwords as ar_words\n",
        "\n",
        "ar_sw=['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'إنها', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما','كان','كانت', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا']\n",
        "\n",
        "def normalizeArabic(t):\n",
        "    t = re.sub(\"[إأٱآا]\", \"ا\", t)\n",
        "    t = re.sub(\"ى\", \"ي\", t)\n",
        "    t = re.sub(\"ة\", 'ه', t)\n",
        "    t = re.sub(\"ؤ\", \"ء\", t)\n",
        "    t = re.sub(\"ئ\", \"ء\", t)\n",
        "    return (t)\n",
        "\n",
        "ar_stop= []\n",
        "for w in ar_sw:\n",
        "  ar_stop.append(normalizeArabic(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAhj6JegwFkk",
        "outputId": "f9ee4a1f-5b20-4295-d02f-63c816198917"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#COLLECT Ar stopwords from 2 sources\n",
        "\n",
        "ar_stopwords = stopwords.words('arabic') + list(ar_words.stopwords_list()) + ar_stop\n",
        "print('nlkt arabic stopwords =',len(stopwords.words('arabic')))\n",
        "print('Arabic-Stopwords =', len(ar_words.stopwords_list()))\n",
        "print('My list =', len(ar_stop))\n",
        "print('sum =',len(ar_stopwords), 'unique=', len(set(ar_stopwords)) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6iw-telwIEN",
        "outputId": "cd25ed4d-4782-4970-a009-90f3d63292a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nlkt arabic stopwords = 754\n",
            "Arabic-Stopwords = 13629\n",
            "My list = 251\n",
            "sum = 14634 unique= 13997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define 'clean_tweet' function to clean the text and remove unwanted text parts\n",
        "def clean_text(text):\n",
        "    # define regular expression patterns\n",
        "    p_english = \"[a-zA-Z0-9]+\"\n",
        "    p_url = \"https?://[A-Za-z0-9./]+\"\n",
        "    p_mention = \"\\@[\\_0-9a-zA-Z]+\\:?\"    \n",
        "    p_retweet = \"RT \\@[\\_\\-0-9a-zA-Z]+\\:?\"\n",
        "    p_punctuations = \"[\" + string.punctuation + \"]\"\n",
        "    \n",
        "    # remove unwanted parts\n",
        "    text = re.sub(p_english, ' ', text)\n",
        "    text = re.sub(p_retweet, ' ', text)\n",
        "    text = re.sub(p_mention, ' ', text)\n",
        "    text = re.sub(p_url, ' ', text)\n",
        "    text = re.sub(p_punctuations, ' ', text)\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    \n",
        "    # remove الهمزة\n",
        "    text = re.sub(\"[أإآ]\", 'ا', text)\n",
        "    text = re.sub(\"ة\", 'ه', text)\n",
        "    text = re.sub(\"ى\", 'ي', text)\n",
        "    \n",
        "    # removing tashkeel\n",
        "    tashkel = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(tashkel, '', text)\n",
        "    \n",
        "    # remove repeated letters more than two letters\n",
        "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "    \n",
        "    #trim    \n",
        "    text = text.strip()\n",
        "    \n",
        "    # remove stopwords\n",
        "    words = [word for word in text.split() if word not in ar_stopwords]\n",
        "    words = [word for word in words if len(word)>=2]\n",
        "    \n",
        "    # merge and return final text\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "5h43JfqCwKfd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply clean function on all text in the dataframes\n",
        "for k, v in datasetDict.items():\n",
        "    v['clean_text'] = v['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "06RkXw7SwM0X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#------------------------------------------------------ Choose a dataset df\n",
        "df= res_df\n",
        "#df= prod_df\n",
        "#df= htl_df\n",
        "#df= mov_df\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "#max_vocab = 1500 # max number of words to consider when tokenizing (based on freq)\n",
        "\n",
        "# fit tokenizer vocab (note that it lowercases and strips punct)\n",
        "#tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.text)\n",
        "max_vocab = len(tokenizer.word_index) +1\n",
        "# standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len)\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len)\n",
        "\n",
        "\n",
        "embedding_dim = 20 # hyper-parameter \n",
        "\n",
        "inp = Input(shape=(seq_len,)) # must specify format of input layer\n",
        "x = Embedding(max_vocab, embedding_dim)(inp) # model learns its own word embeddings\n",
        "x = Bidirectional(LSTM(8, recurrent_dropout=.3))(x) # bi-LSTM with regularization\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "NN.summary()\n",
        "\n",
        "threshold= 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "history = NN.fit(train_text, y_train, \n",
        "                 validation_data=(val_text, y_val),\n",
        "                 epochs=25, batch_size=512, verbose=1)\n",
        "\n",
        "\n",
        "print('RESULT: model f1 score is :', f1_score(y_val, (NN.predict(val_text)[:,0] > .5).astype(int)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoy8TGZGs8AC",
        "outputId": "107bb0bf-bd48-4e31-f9b9-cd17e429ae3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, 128, 20)           1151340   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 16)               1856      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,153,213\n",
            "Trainable params: 1,153,213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "17/17 [==============================] - 28s 1s/step - loss: 0.6529 - accuracy: 0.7498 - f1_score: 0.8570 - val_loss: 0.6126 - val_accuracy: 0.7501 - val_f1_score: 0.8572\n",
            "Epoch 2/25\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.5754 - accuracy: 0.7501 - f1_score: 0.8572 - val_loss: 0.5515 - val_accuracy: 0.7501 - val_f1_score: 0.8572\n",
            "Epoch 3/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.5398 - accuracy: 0.7501 - f1_score: 0.8572 - val_loss: 0.5377 - val_accuracy: 0.7501 - val_f1_score: 0.8572\n",
            "Epoch 4/25\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.5061 - accuracy: 0.7501 - f1_score: 0.8572 - val_loss: 0.5071 - val_accuracy: 0.7501 - val_f1_score: 0.8572\n",
            "Epoch 5/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.4412 - accuracy: 0.7505 - f1_score: 0.8574 - val_loss: 0.4550 - val_accuracy: 0.7511 - val_f1_score: 0.8577\n",
            "Epoch 6/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.3657 - accuracy: 0.7783 - f1_score: 0.8712 - val_loss: 0.4253 - val_accuracy: 0.7753 - val_f1_score: 0.8698\n",
            "Epoch 7/25\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.3203 - accuracy: 0.8558 - f1_score: 0.9123 - val_loss: 0.4175 - val_accuracy: 0.8043 - val_f1_score: 0.8840\n",
            "Epoch 8/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.2830 - accuracy: 0.8956 - f1_score: 0.9349 - val_loss: 0.4114 - val_accuracy: 0.8169 - val_f1_score: 0.8905\n",
            "Epoch 9/25\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.2592 - accuracy: 0.9360 - f1_score: 0.9589 - val_loss: 0.4080 - val_accuracy: 0.8407 - val_f1_score: 0.9025\n",
            "Epoch 10/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.2351 - accuracy: 0.9499 - f1_score: 0.9675 - val_loss: 0.4107 - val_accuracy: 0.8454 - val_f1_score: 0.9047\n",
            "Epoch 11/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.2141 - accuracy: 0.9597 - f1_score: 0.9737 - val_loss: 0.4076 - val_accuracy: 0.8552 - val_f1_score: 0.9083\n",
            "Epoch 12/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.2026 - accuracy: 0.9726 - f1_score: 0.9819 - val_loss: 0.4024 - val_accuracy: 0.8543 - val_f1_score: 0.9083\n",
            "Epoch 13/25\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1906 - accuracy: 0.9724 - f1_score: 0.9817 - val_loss: 0.4096 - val_accuracy: 0.8515 - val_f1_score: 0.9047\n",
            "Epoch 14/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1846 - accuracy: 0.9726 - f1_score: 0.9817 - val_loss: 0.4072 - val_accuracy: 0.8333 - val_f1_score: 0.8888\n",
            "Epoch 15/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1826 - accuracy: 0.9703 - f1_score: 0.9801 - val_loss: 0.4387 - val_accuracy: 0.8375 - val_f1_score: 0.8949\n",
            "Epoch 16/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1646 - accuracy: 0.9708 - f1_score: 0.9805 - val_loss: 0.4238 - val_accuracy: 0.8449 - val_f1_score: 0.8997\n",
            "Epoch 17/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1439 - accuracy: 0.9783 - f1_score: 0.9855 - val_loss: 0.4219 - val_accuracy: 0.8515 - val_f1_score: 0.9060\n",
            "Epoch 18/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1267 - accuracy: 0.9805 - f1_score: 0.9870 - val_loss: 0.4033 - val_accuracy: 0.8379 - val_f1_score: 0.8929\n",
            "Epoch 19/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1156 - accuracy: 0.9794 - f1_score: 0.9862 - val_loss: 0.4112 - val_accuracy: 0.8421 - val_f1_score: 0.8968\n",
            "Epoch 20/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1125 - accuracy: 0.9804 - f1_score: 0.9868 - val_loss: 0.4289 - val_accuracy: 0.8477 - val_f1_score: 0.9016\n",
            "Epoch 21/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1058 - accuracy: 0.9819 - f1_score: 0.9879 - val_loss: 0.4199 - val_accuracy: 0.8440 - val_f1_score: 0.8982\n",
            "Epoch 22/25\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0949 - accuracy: 0.9845 - f1_score: 0.9896 - val_loss: 0.4284 - val_accuracy: 0.8468 - val_f1_score: 0.9023\n",
            "Epoch 23/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.0872 - accuracy: 0.9853 - f1_score: 0.9901 - val_loss: 0.4160 - val_accuracy: 0.8384 - val_f1_score: 0.8937\n",
            "Epoch 24/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.0853 - accuracy: 0.9846 - f1_score: 0.9897 - val_loss: 0.4328 - val_accuracy: 0.8398 - val_f1_score: 0.8951\n",
            "Epoch 25/25\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.0781 - accuracy: 0.9861 - f1_score: 0.9907 - val_loss: 0.4482 - val_accuracy: 0.8487 - val_f1_score: 0.9026\n",
            "RESULT: model f1 score is : 0.9026442307692308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#------------------------------------------------------ Choose a dataset df\n",
        "#df= res_df\n",
        "df= prod_df\n",
        "#df= htl_df\n",
        "#df= mov_df\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "#max_vocab = 1500 # max number of words to consider when tokenizing (based on freq)\n",
        "\n",
        "# fit tokenizer vocab (note that it lowercases and strips punct)\n",
        "#tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.text)\n",
        "max_vocab = len(tokenizer.word_index) +1\n",
        "# standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len)\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len)\n",
        "\n",
        "\n",
        "embedding_dim = 20 # hyper-parameter \n",
        "\n",
        "inp = Input(shape=(seq_len,)) # must specify format of input layer\n",
        "x = Embedding(max_vocab, embedding_dim)(inp) # model learns its own word embeddings\n",
        "x = Bidirectional(LSTM(8, recurrent_dropout=.3))(x) # bi-LSTM with regularization\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "NN.summary()\n",
        "\n",
        "threshold= 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "history = NN.fit(train_text, y_train, \n",
        "                 validation_data=(val_text, y_val),\n",
        "                 epochs=25, batch_size=512, verbose=1)\n",
        "\n",
        "\n",
        "print('RESULT: model f1 score is :', f1_score(y_val, (NN.predict(val_text)[:,0] > .5).astype(int)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e499ik5Wwx7Z",
        "outputId": "98bfa045-01bc-4689-80d2-3690b6f68f4c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, 128, 20)           257680    \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 16)               1856      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 259,553\n",
            "Trainable params: 259,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "7/7 [==============================] - 15s 1s/step - loss: 0.6779 - accuracy: 0.7815 - f1_score: 0.8773 - val_loss: 0.6626 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 2/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.6528 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.6375 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 3/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.6253 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.6091 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 4/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.5949 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.5777 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 5/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.5619 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.5470 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 6/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.5320 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.5237 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 7/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.5116 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.5148 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 8/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.5007 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.5145 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 9/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.4971 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.5116 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 10/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.4875 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.5051 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 11/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.4761 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.4991 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 12/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.4638 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.4923 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 13/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.4480 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.4838 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 14/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.4295 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.4740 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 15/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.4072 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.4642 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 16/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.3867 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.4554 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 17/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.3664 - accuracy: 0.7824 - f1_score: 0.8779 - val_loss: 0.4479 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 18/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.3480 - accuracy: 0.7827 - f1_score: 0.8781 - val_loss: 0.4415 - val_accuracy: 0.7818 - val_f1_score: 0.8776\n",
            "Epoch 19/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.3321 - accuracy: 0.7887 - f1_score: 0.8810 - val_loss: 0.4361 - val_accuracy: 0.7831 - val_f1_score: 0.8782\n",
            "Epoch 20/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.3176 - accuracy: 0.8146 - f1_score: 0.8941 - val_loss: 0.4320 - val_accuracy: 0.7894 - val_f1_score: 0.8813\n",
            "Epoch 21/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.3025 - accuracy: 0.8404 - f1_score: 0.9075 - val_loss: 0.4301 - val_accuracy: 0.7982 - val_f1_score: 0.8857\n",
            "Epoch 22/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.2912 - accuracy: 0.8735 - f1_score: 0.9252 - val_loss: 0.4261 - val_accuracy: 0.7970 - val_f1_score: 0.8849\n",
            "Epoch 23/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.2808 - accuracy: 0.8824 - f1_score: 0.9301 - val_loss: 0.4271 - val_accuracy: 0.8121 - val_f1_score: 0.8924\n",
            "Epoch 24/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.2758 - accuracy: 0.9164 - f1_score: 0.9492 - val_loss: 0.4240 - val_accuracy: 0.8159 - val_f1_score: 0.8944\n",
            "Epoch 25/25\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.2624 - accuracy: 0.9158 - f1_score: 0.9489 - val_loss: 0.4196 - val_accuracy: 0.8184 - val_f1_score: 0.8957\n",
            "RESULT: model f1 score is : 0.8956521739130434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#------------------------------------------------------ Choose a dataset df\n",
        "#df= res_df\n",
        "#df= prod_df\n",
        "df= htl_df\n",
        "#df= mov_df\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "#max_vocab = 1500 # max number of words to consider when tokenizing (based on freq)\n",
        "\n",
        "# fit tokenizer vocab \n",
        "#tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.text)\n",
        "max_vocab = len(tokenizer.word_index) +1\n",
        "# standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len)\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len)\n",
        "\n",
        "\n",
        "embedding_dim = 20 # hyper-parameter \n",
        "\n",
        "inp = Input(shape=(seq_len,)) # must specify format of input layer\n",
        "x = Embedding(max_vocab, embedding_dim)(inp) # model learns its own word embeddings\n",
        "x = Bidirectional(LSTM(8, recurrent_dropout=.3))(x) # bi-LSTM with regularization\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "NN.summary()\n",
        "\n",
        "threshold= 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "history = NN.fit(train_text, y_train, \n",
        "                 validation_data=(val_text, y_val),\n",
        "                 epochs=25, batch_size=512, verbose=1)\n",
        "\n",
        "\n",
        "print('RESULT: model f1 score is :', f1_score(y_val, (NN.predict(val_text)[:,0] > .5).astype(int)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdK1j224d2zm",
        "outputId": "75581692-b4f4-4d21-d9a8-007a4f9b0fc7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     (None, 128, 20)           2171700   \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 16)               1856      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,173,573\n",
            "Trainable params: 2,173,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "21/21 [==============================] - 35s 1s/step - loss: 0.6344 - accuracy: 0.7996 - f1_score: 0.8886 - val_loss: 0.5685 - val_accuracy: 0.8026 - val_f1_score: 0.8905\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.5135 - accuracy: 0.8028 - f1_score: 0.8906 - val_loss: 0.4795 - val_accuracy: 0.8026 - val_f1_score: 0.8905\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.4673 - accuracy: 0.8028 - f1_score: 0.8906 - val_loss: 0.4569 - val_accuracy: 0.8026 - val_f1_score: 0.8905\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 29s 1s/step - loss: 0.4119 - accuracy: 0.8028 - f1_score: 0.8906 - val_loss: 0.3853 - val_accuracy: 0.8026 - val_f1_score: 0.8905\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.3235 - accuracy: 0.8106 - f1_score: 0.8945 - val_loss: 0.3193 - val_accuracy: 0.8089 - val_f1_score: 0.8936\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.2669 - accuracy: 0.8237 - f1_score: 0.9011 - val_loss: 0.2903 - val_accuracy: 0.8182 - val_f1_score: 0.8982\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.2274 - accuracy: 0.8878 - f1_score: 0.9347 - val_loss: 0.2685 - val_accuracy: 0.8793 - val_f1_score: 0.9297\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 26s 1s/step - loss: 0.1979 - accuracy: 0.9700 - f1_score: 0.9816 - val_loss: 0.2506 - val_accuracy: 0.9117 - val_f1_score: 0.9470\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.1794 - accuracy: 0.9783 - f1_score: 0.9866 - val_loss: 0.2457 - val_accuracy: 0.9210 - val_f1_score: 0.9524\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.1592 - accuracy: 0.9896 - f1_score: 0.9935 - val_loss: 0.2369 - val_accuracy: 0.9304 - val_f1_score: 0.9574\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 29s 1s/step - loss: 0.1415 - accuracy: 0.9942 - f1_score: 0.9964 - val_loss: 0.2381 - val_accuracy: 0.9304 - val_f1_score: 0.9575\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 51s 2s/step - loss: 0.1276 - accuracy: 0.9945 - f1_score: 0.9966 - val_loss: 0.2210 - val_accuracy: 0.9326 - val_f1_score: 0.9584\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 51s 2s/step - loss: 0.1137 - accuracy: 0.9958 - f1_score: 0.9974 - val_loss: 0.2160 - val_accuracy: 0.9352 - val_f1_score: 0.9599\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 55s 3s/step - loss: 0.1026 - accuracy: 0.9970 - f1_score: 0.9981 - val_loss: 0.2128 - val_accuracy: 0.9337 - val_f1_score: 0.9589\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 39s 2s/step - loss: 0.0927 - accuracy: 0.9972 - f1_score: 0.9983 - val_loss: 0.2129 - val_accuracy: 0.9359 - val_f1_score: 0.9604\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.0845 - accuracy: 0.9967 - f1_score: 0.9980 - val_loss: 0.2074 - val_accuracy: 0.9296 - val_f1_score: 0.9561\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.0758 - accuracy: 0.9973 - f1_score: 0.9983 - val_loss: 0.2066 - val_accuracy: 0.9315 - val_f1_score: 0.9573\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.0688 - accuracy: 0.9976 - f1_score: 0.9985 - val_loss: 0.2030 - val_accuracy: 0.9352 - val_f1_score: 0.9597\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.0655 - accuracy: 0.9964 - f1_score: 0.9977 - val_loss: 0.2070 - val_accuracy: 0.9236 - val_f1_score: 0.9517\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.0587 - accuracy: 0.9975 - f1_score: 0.9984 - val_loss: 0.2060 - val_accuracy: 0.9318 - val_f1_score: 0.9576\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.0537 - accuracy: 0.9978 - f1_score: 0.9986 - val_loss: 0.2022 - val_accuracy: 0.9304 - val_f1_score: 0.9564\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.0493 - accuracy: 0.9980 - f1_score: 0.9988 - val_loss: 0.2020 - val_accuracy: 0.9304 - val_f1_score: 0.9565\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.0445 - accuracy: 0.9984 - f1_score: 0.9990 - val_loss: 0.2030 - val_accuracy: 0.9304 - val_f1_score: 0.9563\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 26s 1s/step - loss: 0.0412 - accuracy: 0.9983 - f1_score: 0.9990 - val_loss: 0.1962 - val_accuracy: 0.9371 - val_f1_score: 0.9609\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 27s 1s/step - loss: 0.0387 - accuracy: 0.9980 - f1_score: 0.9987 - val_loss: 0.2061 - val_accuracy: 0.9255 - val_f1_score: 0.9529\n",
            "RESULT: model f1 score is : 0.9528524280999529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#------------------------------------------------------ Choose a dataset df\n",
        "#df= res_df\n",
        "#df= prod_df\n",
        "#df= htl_df\n",
        "df= mov_df\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "#max_vocab = 1500 # max number of words to consider when tokenizing (based on freq)\n",
        "\n",
        "# fit tokenizer vocab \n",
        "#tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.text)\n",
        "max_vocab = len(tokenizer.word_index) +1\n",
        "# standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len)\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len)\n",
        "\n",
        "\n",
        "embedding_dim = 20 # hyper-parameter \n",
        "\n",
        "inp = Input(shape=(seq_len,)) # must specify format of input layer\n",
        "x = Embedding(max_vocab, embedding_dim)(inp) # model learns its own word embeddings\n",
        "x = Bidirectional(LSTM(8, recurrent_dropout=.3))(x) # bi-LSTM with regularization\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "NN.summary()\n",
        "\n",
        "threshold= 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "history = NN.fit(train_text, y_train, \n",
        "                 validation_data=(val_text, y_val),\n",
        "                 epochs=25, batch_size=512, verbose=1)\n",
        "\n",
        "\n",
        "print('RESULT: model f1 score is :', f1_score(y_val, (NN.predict(val_text)[:,0] > .5).astype(int)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aokokk3AhF4N",
        "outputId": "48ae0fda-4db2-415e-8e26-5a09eae42218"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, 128, 20)           1489240   \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 16)               1856      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,491,113\n",
            "Trainable params: 1,491,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "3/3 [==============================] - 15s 2s/step - loss: 0.6916 - accuracy: 0.5638 - f1_score: 0.6853 - val_loss: 0.6873 - val_accuracy: 0.7048 - val_f1_score: 0.8246\n",
            "Epoch 2/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.6838 - accuracy: 0.7311 - f1_score: 0.8402 - val_loss: 0.6823 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 3/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.6764 - accuracy: 0.7181 - f1_score: 0.8356 - val_loss: 0.6771 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 4/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.6684 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6716 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 5/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.6599 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6658 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 6/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.6509 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6598 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 7/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.6409 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6535 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 8/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.6304 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6467 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 9/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.6191 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6396 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 10/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.6063 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6324 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 11/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.5934 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6252 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 12/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.5792 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6180 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 13/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.5632 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6110 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 14/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.5474 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.6046 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 15/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.5310 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.5992 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 16/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.5144 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.5956 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 17/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.4980 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.5942 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 18/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.4828 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.5941 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 19/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.4683 - accuracy: 0.7163 - f1_score: 0.8347 - val_loss: 0.5935 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 20/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.4491 - accuracy: 0.7172 - f1_score: 0.8351 - val_loss: 0.5920 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 21/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.4260 - accuracy: 0.7237 - f1_score: 0.8383 - val_loss: 0.5886 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 22/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.4010 - accuracy: 0.7431 - f1_score: 0.8479 - val_loss: 0.5828 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 23/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.3745 - accuracy: 0.7800 - f1_score: 0.8669 - val_loss: 0.5758 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 24/25\n",
            "3/3 [==============================] - 5s 2s/step - loss: 0.3469 - accuracy: 0.8253 - f1_score: 0.8913 - val_loss: 0.5682 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "Epoch 25/25\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.3165 - accuracy: 0.8725 - f1_score: 0.9182 - val_loss: 0.5603 - val_accuracy: 0.7159 - val_f1_score: 0.8344\n",
            "RESULT: model f1 score is : 0.8344086021505376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#------------------------------------------------------ Choose a dataset df\n",
        "#df= res_df\n",
        "#df= prod_df\n",
        "#df= htl_df\n",
        "#df= mov_df\n",
        "\n",
        "df = pd.concat([res_df,prod_df,htl_df,mov_df])\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "#max_vocab = 1500 # max number of words to consider when tokenizing (based on freq)\n",
        "\n",
        "# fit tokenizer vocab \n",
        "#tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.text)\n",
        "max_vocab = len(tokenizer.word_index) +1\n",
        "# standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len)\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len)\n",
        "\n",
        "\n",
        "embedding_dim = 20 # hyper-parameter \n",
        "\n",
        "inp = Input(shape=(seq_len,)) # must specify format of input layer\n",
        "x = Embedding(max_vocab, embedding_dim)(inp) # model learns its own word embeddings\n",
        "x = Bidirectional(LSTM(8, recurrent_dropout=.3))(x) # bi-LSTM with regularization\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "NN.summary()\n",
        "\n",
        "threshold= 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "history = NN.fit(train_text, y_train, \n",
        "                 validation_data=(val_text, y_val),\n",
        "                 epochs=25, batch_size=512, verbose=1)\n",
        "\n",
        "\n",
        "print('RESULT: model f1 score is :', f1_score(y_val, (NN.predict(val_text)[:,0] > .5).astype(int)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-DBovRth9Od",
        "outputId": "b52c4185-8be2-4c22-918d-3c464a8a772c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_8 (Embedding)     (None, 128, 20)           3765580   \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 16)               1856      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,767,453\n",
            "Trainable params: 3,767,453\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "47/47 [==============================] - 67s 1s/step - loss: 0.5948 - accuracy: 0.7593 - f1_score: 0.8622 - val_loss: 0.5179 - val_accuracy: 0.7769 - val_f1_score: 0.8744\n",
            "Epoch 2/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.4821 - accuracy: 0.7769 - f1_score: 0.8744 - val_loss: 0.4305 - val_accuracy: 0.7769 - val_f1_score: 0.8744\n",
            "Epoch 3/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.3457 - accuracy: 0.8092 - f1_score: 0.8905 - val_loss: 0.3435 - val_accuracy: 0.8248 - val_f1_score: 0.8982\n",
            "Epoch 4/25\n",
            "47/47 [==============================] - 60s 1s/step - loss: 0.2598 - accuracy: 0.9201 - f1_score: 0.9508 - val_loss: 0.3244 - val_accuracy: 0.8614 - val_f1_score: 0.9169\n",
            "Epoch 5/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.2057 - accuracy: 0.9570 - f1_score: 0.9728 - val_loss: 0.3127 - val_accuracy: 0.8845 - val_f1_score: 0.9287\n",
            "Epoch 6/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.1668 - accuracy: 0.9683 - f1_score: 0.9798 - val_loss: 0.3168 - val_accuracy: 0.8845 - val_f1_score: 0.9290\n",
            "Epoch 7/25\n",
            "47/47 [==============================] - 62s 1s/step - loss: 0.1395 - accuracy: 0.9725 - f1_score: 0.9824 - val_loss: 0.3087 - val_accuracy: 0.8818 - val_f1_score: 0.9249\n",
            "Epoch 8/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.1156 - accuracy: 0.9776 - f1_score: 0.9857 - val_loss: 0.2979 - val_accuracy: 0.8896 - val_f1_score: 0.9306\n",
            "Epoch 9/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0974 - accuracy: 0.9810 - f1_score: 0.9879 - val_loss: 0.3066 - val_accuracy: 0.8895 - val_f1_score: 0.9308\n",
            "Epoch 10/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0832 - accuracy: 0.9835 - f1_score: 0.9894 - val_loss: 0.3382 - val_accuracy: 0.8874 - val_f1_score: 0.9303\n",
            "Epoch 11/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0714 - accuracy: 0.9868 - f1_score: 0.9916 - val_loss: 0.3257 - val_accuracy: 0.8884 - val_f1_score: 0.9297\n",
            "Epoch 12/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0643 - accuracy: 0.9872 - f1_score: 0.9918 - val_loss: 0.3376 - val_accuracy: 0.8874 - val_f1_score: 0.9295\n",
            "Epoch 13/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0582 - accuracy: 0.9884 - f1_score: 0.9925 - val_loss: 0.3386 - val_accuracy: 0.8896 - val_f1_score: 0.9306\n",
            "Epoch 14/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0522 - accuracy: 0.9894 - f1_score: 0.9932 - val_loss: 0.3481 - val_accuracy: 0.8896 - val_f1_score: 0.9306\n",
            "Epoch 15/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0472 - accuracy: 0.9908 - f1_score: 0.9941 - val_loss: 0.4647 - val_accuracy: 0.8256 - val_f1_score: 0.8805\n",
            "Epoch 16/25\n",
            "47/47 [==============================] - 60s 1s/step - loss: 0.0509 - accuracy: 0.9890 - f1_score: 0.9929 - val_loss: 0.3880 - val_accuracy: 0.8769 - val_f1_score: 0.9232\n",
            "Epoch 17/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0428 - accuracy: 0.9910 - f1_score: 0.9943 - val_loss: 0.3783 - val_accuracy: 0.8820 - val_f1_score: 0.9259\n",
            "Epoch 18/25\n",
            "47/47 [==============================] - 60s 1s/step - loss: 0.0382 - accuracy: 0.9924 - f1_score: 0.9951 - val_loss: 0.3935 - val_accuracy: 0.8840 - val_f1_score: 0.9272\n",
            "Epoch 19/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0355 - accuracy: 0.9924 - f1_score: 0.9951 - val_loss: 0.3937 - val_accuracy: 0.8852 - val_f1_score: 0.9279\n",
            "Epoch 20/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0333 - accuracy: 0.9930 - f1_score: 0.9955 - val_loss: 0.4214 - val_accuracy: 0.8850 - val_f1_score: 0.9281\n",
            "Epoch 21/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0319 - accuracy: 0.9930 - f1_score: 0.9955 - val_loss: 0.4124 - val_accuracy: 0.8849 - val_f1_score: 0.9277\n",
            "Epoch 22/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0293 - accuracy: 0.9933 - f1_score: 0.9957 - val_loss: 0.4369 - val_accuracy: 0.8827 - val_f1_score: 0.9265\n",
            "Epoch 23/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0281 - accuracy: 0.9936 - f1_score: 0.9959 - val_loss: 0.4274 - val_accuracy: 0.8827 - val_f1_score: 0.9259\n",
            "Epoch 24/25\n",
            "47/47 [==============================] - 61s 1s/step - loss: 0.0275 - accuracy: 0.9933 - f1_score: 0.9957 - val_loss: 0.4387 - val_accuracy: 0.8823 - val_f1_score: 0.9258\n",
            "Epoch 25/25\n",
            "47/47 [==============================] - 62s 1s/step - loss: 0.0257 - accuracy: 0.9937 - f1_score: 0.9959 - val_loss: 0.4432 - val_accuracy: 0.8825 - val_f1_score: 0.9258\n",
            "RESULT: model f1 score is : 0.9257510729613734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ArSenTD-Lev (Arabic Sentiment Twitter Dataset for LEVantine dialect)**"
      ],
      "metadata": {
        "id": "lnyaCnapfHXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read files of first dataset (MSA)\n",
        "cor_df=pd.read_csv('ArSenTD-LEV.tsv', sep='\\t')\n",
        "cor_df= cor_df.drop(columns=['Country', 'Sentiment_Expression','Sentiment_Target'])\n",
        "#keep only binary classes (pos & neg)\n",
        "cor_df=cor_df[cor_df['Sentiment']!='neutral'].reset_index(drop=True)\n",
        "#unify format with other dataset\n",
        "cor_df['Sentiment'].replace({'very_positive': '1', 'positive': '1', 'very_negative':'0', 'negative':'0'}, inplace=True)\n",
        "cor_df= cor_df.reset_index(drop=True)\n",
        "cor_df.rename(columns={'Tweet': 'text', 'Sentiment': 'polarity'}, inplace=True)\n",
        "cor_df[\"polarity\"] = pd.to_numeric(cor_df[\"polarity\"])\n",
        "cor_df.head()\n",
        "cor_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "lZvrmWWMfFbf",
        "outputId": "d584ec90-9d5f-4ae7-f2a4-eee0a063ac09"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Topic</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"أنا أؤمن بأن الانسان ينطفئ جماله عند ابتعاد م...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>من الذاكره... @3FInQe . عندما اعتقد كريستيانو ...</td>\n",
              "      <td>sports</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#مصطلحات_لبنانيه_حيرت_البشريه بتوصل عالبيت ، ب...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>نصمت !! لتسير حياتنا على مً يرام فالناّس لم تع...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Yousef_MUFC اكثر ما يزعجنا بعد مستوانا خارج ا...</td>\n",
              "      <td>sports</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3110</th>\n",
              "      <td>نهتم من خلال خدمة تنسيق الرسائل بإظهار رسالة ا...</td>\n",
              "      <td>education</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3111</th>\n",
              "      <td>صلاح من لاعب في المقاولون العرب يحلم ان يلعب ل...</td>\n",
              "      <td>sports</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3112</th>\n",
              "      <td>الملك سلمان بن عبد العزيز: تطبيق الأنظمة بحزم ...</td>\n",
              "      <td>politics</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3113</th>\n",
              "      <td>@ZahraaIraq9 😂 كل ما ادخل حسابي الكه تغريداتج ...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3114</th>\n",
              "      <td>شو هالشعب نحنا اللي عايش بلا مي وكهربا والزبال...</td>\n",
              "      <td>politics</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3115 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text      Topic  polarity\n",
              "0     \"أنا أؤمن بأن الانسان ينطفئ جماله عند ابتعاد م...   personal         0\n",
              "1     من الذاكره... @3FInQe . عندما اعتقد كريستيانو ...     sports         1\n",
              "2     #مصطلحات_لبنانيه_حيرت_البشريه بتوصل عالبيت ، ب...   personal         0\n",
              "3     نصمت !! لتسير حياتنا على مً يرام فالناّس لم تع...   personal         0\n",
              "4     @Yousef_MUFC اكثر ما يزعجنا بعد مستوانا خارج ا...     sports         0\n",
              "...                                                 ...        ...       ...\n",
              "3110  نهتم من خلال خدمة تنسيق الرسائل بإظهار رسالة ا...  education         1\n",
              "3111  صلاح من لاعب في المقاولون العرب يحلم ان يلعب ل...     sports         1\n",
              "3112  الملك سلمان بن عبد العزيز: تطبيق الأنظمة بحزم ...   politics         1\n",
              "3113  @ZahraaIraq9 😂 كل ما ادخل حسابي الكه تغريداتج ...   personal         0\n",
              "3114  شو هالشعب نحنا اللي عايش بلا مي وكهربا والزبال...   politics         0\n",
              "\n",
              "[3115 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                                   u\"\\U00002702-\\U000027B0\"\n",
        "                                   u\"\\U000024C2-\\U0001F251\"\n",
        "                                   \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "sr8UOtDFitB4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define 'clean_tweet' function to clean the text and remove unwanted text parts\n",
        "def clean_tweet(text):\n",
        "    # define regular expression patterns\n",
        "    p_english = \"[a-zA-Z0-9]+\"\n",
        "    p_url = \"https?://[A-Za-z0-9./]+\"\n",
        "    p_mention = \"\\@[\\_0-9a-zA-Z]+\\:?\"    \n",
        "    p_retweet = \"RT \\@[\\_\\-0-9a-zA-Z]+\\:?\"\n",
        "    p_punctuations = \"[\" + string.punctuation + \"]\"\n",
        "    \n",
        "    # remove unwanted parts\n",
        "    text = re.sub(p_english, ' ', text)\n",
        "    text = re.sub(p_retweet, ' ', text)\n",
        "    text = re.sub(p_mention, ' ', text)\n",
        "    text = re.sub(p_url, ' ', text)\n",
        "    text = re.sub(p_punctuations, ' ', text)\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    \n",
        "    # remove الهمزة\n",
        "    text = re.sub(\"[أإآ]\", 'ا', text)\n",
        "    text = re.sub(\"ة\", 'ه', text)\n",
        "    text = re.sub(\"ى\", 'ي', text)\n",
        "    \n",
        "    # removing tashkeel\n",
        "    tashkel = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(tashkel, '', text)\n",
        "    \n",
        "    # remove repeated letters more than two letters\n",
        "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    text= remove_emoji(text)\n",
        "    \n",
        "    # remove stopwords\n",
        "    words = [word for word in text.split() if word not in ar_stopwords]\n",
        "    words = [word for word in words if len(word)>=2]\n",
        "    \n",
        "    # merge and return final text\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "Ap6q8Fsxiyur"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " cor_df['clean_text'] = cor_df['text'].apply(clean_tweet)"
      ],
      "metadata": {
        "id": "LBrTCMVpi9pv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df= cor_df\n",
        "\n",
        "seq_len = 128 # standardized length of each word sequence \n",
        "#max_vocab = 1500 # max number of words to consider when tokenizing (based on freq)\n",
        "\n",
        "# fit tokenizer vocab \n",
        "#tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.text)\n",
        "max_vocab = len(tokenizer.word_index) +1\n",
        "# standard train/val split\n",
        "train_text, val_text, y_train, y_val = train_test_split(df.clean_text, df.polarity, \n",
        "                                                        test_size=0.2, random_state=123, stratify=df.polarity)\n",
        "\n",
        "# convert train and val texts to token sequences of standardized length 128,\n",
        "# padding fills leading 0s in or cuts off sequence at 128th word\n",
        "train_text = tokenizer.texts_to_sequences(train_text) \n",
        "train_text = pad_sequences(train_text, maxlen=seq_len)\n",
        "\n",
        "val_text = tokenizer.texts_to_sequences(val_text)\n",
        "val_text = pad_sequences(val_text, maxlen=seq_len)\n",
        "\n",
        "\n",
        "embedding_dim = 20 # hyper-parameter \n",
        "\n",
        "inp = Input(shape=(seq_len,)) # must specify format of input layer\n",
        "x = Embedding(max_vocab, embedding_dim)(inp) # model learns its own word embeddings\n",
        "x = Bidirectional(LSTM(8, recurrent_dropout=.3))(x) # bi-LSTM with regularization\n",
        "y = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "NN = Model(inp, y)\n",
        "NN.summary()\n",
        "\n",
        "threshold= 0.5\n",
        "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, average='micro', threshold=threshold)])\n",
        "history = NN.fit(train_text, y_train, \n",
        "                 validation_data=(val_text, y_val),\n",
        "                 epochs=25, batch_size=512, verbose=1)\n",
        "\n",
        "\n",
        "print('RESULT: model f1 score is :', f1_score(y_val, (NN.predict(val_text)[:,0] > .5).astype(int)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGqAwXTOfmRv",
        "outputId": "56fc253c-417a-4874-e818-81212ad657d8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_7 (Embedding)     (None, 128, 20)           424260    \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 16)               1856      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 426,133\n",
            "Trainable params: 426,133\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "5/5 [==============================] - 13s 1s/step - loss: 0.6916 - accuracy: 0.5562 - f1_score: 0.4228 - val_loss: 0.6873 - val_accuracy: 0.6051 - val_f1_score: 0.0000e+00\n",
            "Epoch 2/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.6827 - accuracy: 0.6043 - f1_score: 0.0000e+00 - val_loss: 0.6799 - val_accuracy: 0.6051 - val_f1_score: 0.0000e+00\n",
            "Epoch 3/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.6741 - accuracy: 0.6043 - f1_score: 0.0000e+00 - val_loss: 0.6730 - val_accuracy: 0.6051 - val_f1_score: 0.0000e+00\n",
            "Epoch 4/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.6643 - accuracy: 0.6043 - f1_score: 0.0000e+00 - val_loss: 0.6666 - val_accuracy: 0.6051 - val_f1_score: 0.0000e+00\n",
            "Epoch 5/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.6554 - accuracy: 0.6043 - f1_score: 0.0000e+00 - val_loss: 0.6609 - val_accuracy: 0.6051 - val_f1_score: 0.0000e+00\n",
            "Epoch 6/25\n",
            "5/5 [==============================] - 6s 1s/step - loss: 0.6454 - accuracy: 0.6043 - f1_score: 0.0000e+00 - val_loss: 0.6558 - val_accuracy: 0.6051 - val_f1_score: 0.0000e+00\n",
            "Epoch 7/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.6354 - accuracy: 0.6043 - f1_score: 0.0000e+00 - val_loss: 0.6501 - val_accuracy: 0.6051 - val_f1_score: 0.0000e+00\n",
            "Epoch 8/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.6214 - accuracy: 0.6043 - f1_score: 0.0000e+00 - val_loss: 0.6426 - val_accuracy: 0.6051 - val_f1_score: 0.0000e+00\n",
            "Epoch 9/25\n",
            "5/5 [==============================] - 7s 2s/step - loss: 0.6031 - accuracy: 0.6043 - f1_score: 0.0000e+00 - val_loss: 0.6321 - val_accuracy: 0.6051 - val_f1_score: 0.0000e+00\n",
            "Epoch 10/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.5796 - accuracy: 0.6043 - f1_score: 0.0000e+00 - val_loss: 0.6181 - val_accuracy: 0.6051 - val_f1_score: 0.0000e+00\n",
            "Epoch 11/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.5491 - accuracy: 0.6067 - f1_score: 0.0121 - val_loss: 0.5975 - val_accuracy: 0.6083 - val_f1_score: 0.0240\n",
            "Epoch 12/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.5135 - accuracy: 0.6677 - f1_score: 0.2762 - val_loss: 0.5757 - val_accuracy: 0.6501 - val_f1_score: 0.2101\n",
            "Epoch 13/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.4740 - accuracy: 0.7789 - f1_score: 0.6128 - val_loss: 0.5493 - val_accuracy: 0.7127 - val_f1_score: 0.4389\n",
            "Epoch 14/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.4358 - accuracy: 0.8660 - f1_score: 0.7968 - val_loss: 0.5303 - val_accuracy: 0.7592 - val_f1_score: 0.5833\n",
            "Epoch 15/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.4056 - accuracy: 0.9494 - f1_score: 0.9325 - val_loss: 0.5144 - val_accuracy: 0.7464 - val_f1_score: 0.5407\n",
            "Epoch 16/25\n",
            "5/5 [==============================] - 6s 1s/step - loss: 0.3732 - accuracy: 0.9446 - f1_score: 0.9252 - val_loss: 0.4945 - val_accuracy: 0.7849 - val_f1_score: 0.6633\n",
            "Epoch 17/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.3460 - accuracy: 0.9615 - f1_score: 0.9493 - val_loss: 0.4803 - val_accuracy: 0.7994 - val_f1_score: 0.6929\n",
            "Epoch 18/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.3219 - accuracy: 0.9695 - f1_score: 0.9605 - val_loss: 0.4686 - val_accuracy: 0.8010 - val_f1_score: 0.7103\n",
            "Epoch 19/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.3073 - accuracy: 0.9827 - f1_score: 0.9783 - val_loss: 0.4619 - val_accuracy: 0.8058 - val_f1_score: 0.7364\n",
            "Epoch 20/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.2928 - accuracy: 0.9827 - f1_score: 0.9784 - val_loss: 0.4521 - val_accuracy: 0.8042 - val_f1_score: 0.7371\n",
            "Epoch 21/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.2725 - accuracy: 0.9823 - f1_score: 0.9779 - val_loss: 0.4442 - val_accuracy: 0.8058 - val_f1_score: 0.7375\n",
            "Epoch 22/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.2526 - accuracy: 0.9848 - f1_score: 0.9809 - val_loss: 0.4417 - val_accuracy: 0.8122 - val_f1_score: 0.7394\n",
            "Epoch 23/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.2375 - accuracy: 0.9823 - f1_score: 0.9777 - val_loss: 0.4431 - val_accuracy: 0.8090 - val_f1_score: 0.7264\n",
            "Epoch 24/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.2257 - accuracy: 0.9856 - f1_score: 0.9818 - val_loss: 0.4315 - val_accuracy: 0.8122 - val_f1_score: 0.7484\n",
            "Epoch 25/25\n",
            "5/5 [==============================] - 7s 1s/step - loss: 0.2141 - accuracy: 0.9856 - f1_score: 0.9819 - val_loss: 0.4264 - val_accuracy: 0.8122 - val_f1_score: 0.7547\n",
            "RESULT: model f1 score is : 0.7547169811320754\n"
          ]
        }
      ]
    }
  ]
}